# ğŸ§± AI Modular Blocks - æ¨¡å—åŒ–AIå¼€å‘æ¡†æ¶

> "Bad programmers worry about the code. Good programmers worry about data structures and their relationships." - Linus Torvalds

## ğŸ¯ é¡¹ç›®æ¦‚è¿°

### æ ¸å¿ƒç†å¿µ
æ„å»ºä¸€ä¸ªç±»ä¼¼**shadcn/ui**çš„æ¨¡å—åŒ–AIå¼€å‘ä½“ç³» - æä¾›å¯ç»„åˆã€å¯æ›¿æ¢çš„AIåŠŸèƒ½æ„å»ºå—ï¼Œè®©å¼€å‘è€…åƒæ­ä¹é«˜ä¸€æ ·æ„å»ºAIåº”ç”¨ã€‚

### ä¸ºä»€ä¹ˆé€‰æ‹©æ¨¡å—åŒ–è€Œä¸æ˜¯æ¡†æ¶ï¼Ÿ

| ä¼ ç»ŸAIæ¡†æ¶ | AI Modular Blocks |
|------------|-------------------|
| âŒ æ¡†æ¶é”å®šï¼Œéš¾ä»¥æ›¿æ¢ç»„ä»¶ | âœ… å®Œå…¨æ§åˆ¶ï¼Œéšæ—¶æ›¿æ¢ |
| âŒ é»‘ç›’å®ç°ï¼Œè°ƒè¯•å›°éš¾ | âœ… é€æ˜å®ç°ï¼Œå®Œå…¨å¯æ§ |
| âŒ ç»Ÿä¸€ç“¶é¢ˆï¼Œæ€§èƒ½å—é™ | âœ… é’ˆå¯¹æ€§ä¼˜åŒ–ï¼Œæ€§èƒ½æœ€ä¼˜ |
| âŒ ç‰ˆæœ¬å‡çº§é£é™©é«˜ | âœ… æ¸è¿›å¼å‡çº§ï¼Œç¨³å®šå¯é  |

## ğŸ¤” è®¾è®¡å‡ºå‘ç‚¹ï¼šä¸ºä»€ä¹ˆé‡æ–°å‘æ˜è½®å­ï¼Ÿ

### ç°æœ‰AIæ¡†æ¶ç”Ÿæ€çš„æ ¹æœ¬é—®é¢˜

æˆ‘ä»¬åœ¨æ„å»ºç”Ÿäº§çº§AIåº”ç”¨æ—¶ï¼Œå‘ç°ç°æœ‰ä¸»æµæ¡†æ¶éƒ½å­˜åœ¨æ ¹æœ¬æ€§çš„è®¾è®¡ç¼ºé™·ã€‚è®©æˆ‘ä»¬é€ä¸€åˆ†æï¼š

## ğŸ“Š ä¸»æµæ¡†æ¶å¯¹æ¯”åˆ†æ

### 1. LangChainï¼šè¿‡åº¦æŠ½è±¡çš„"é“¾å¼åœ°ç‹±"

#### LangChainçš„æ ¹æœ¬é—®é¢˜

**è¿‡åº¦æŠ½è±¡å¯¼è‡´çš„å¤æ‚æ€§**ï¼š
```python
# ä¸ºäº†åšä¸ªç®€å•çš„LLMè°ƒç”¨ï¼Œä½ éœ€è¦åˆ›å»ºä¸€å †å¯¹è±¡
from langchain.chains import LLMChain, SimpleSequentialChain
from langchain.prompts import PromptTemplate

prompt = PromptTemplate(...)                    # 1. åˆ›å»ºPromptæ¨¡æ¿
llm_chain = LLMChain(llm=llm, prompt=prompt)   # 2. åˆ›å»ºChain
seq_chain = SimpleSequentialChain(chains=[llm_chain])  # 3. åŒ…è£…æˆSequential
result = seq_chain.run(input_text)             # 4. é»‘ç›’æ‰§è¡Œ

# ğŸ¤·â€â™‚ï¸ ä½ æ ¹æœ¬ä¸çŸ¥é“å†…éƒ¨å‘ç”Ÿäº†ä»€ä¹ˆï¼
```

**è°ƒè¯•å›°éš¾çš„é»‘ç›’è®¾è®¡**ï¼š
```python
try:
    result = complex_chain.run(input_text)
except Exception as e:
    # ğŸ˜± é”™è¯¯å¯èƒ½æ¥è‡ªChainçš„ä»»ä½•ç¯èŠ‚ï¼Œè°ƒè¯•nightmare
    print(f"Chain failed: {e}")
```

**æ€§èƒ½å¼€é”€çš„æŠ½è±¡ç¨**ï¼š
```
ç”¨æˆ·è°ƒç”¨ â†’ Chain.run() â†’ LLMChain._call() â†’ BaseLLM.generate() â†’ å®é™…APIè°ƒç”¨
         â¬†ï¸ 4å±‚æŠ½è±¡ + æ¡†æ¶å¼€é”€
```

### 2. LangGraphï¼šå¤æ‚çŠ¶æ€ç®¡ç†çš„æ–°é™·é˜±

LangGraphè¯•å›¾è§£å†³LangChainçš„ä¸€äº›é—®é¢˜ï¼Œä½†å¼•å…¥äº†æ›´å¤šå¤æ‚æ€§ï¼š

#### LangGraphçš„é—®é¢˜

**è¿‡åº¦å·¥ç¨‹åŒ–çš„çŠ¶æ€ç®¡ç†**ï¼š
```python
from langgraph.graph import StateGraph
from langgraph.checkpoint.memory import MemorySaver

# ä¸ºäº†å®ç°ä¸€ä¸ªç®€å•çš„å¯¹è¯ï¼Œä½ éœ€è¦ï¼š
def create_graph():
    workflow = StateGraph(AgentState)
    
    # å®šä¹‰èŠ‚ç‚¹
    workflow.add_node("agent", call_model)
    workflow.add_node("action", call_tool)
    
    # å®šä¹‰è¾¹
    workflow.add_edge(START, "agent")
    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {"continue": "action", "end": END}
    )
    workflow.add_edge("action", "agent")
    
    # ç¼–è¯‘å›¾
    checkpointer = MemorySaver()
    app = workflow.compile(checkpointer=checkpointer)
    return app

# ğŸ¤¯ ä¸ºäº†ä¸€ä¸ªå¯¹è¯ï¼Œéœ€è¦ç†è§£å›¾ã€èŠ‚ç‚¹ã€è¾¹ã€çŠ¶æ€ã€æ£€æŸ¥ç‚¹...
```

**å¤æ‚çš„çŠ¶æ€åŒæ­¥é—®é¢˜**ï¼š
```python
# LangGraphä¸­çŠ¶æ€ç®¡ç†çš„å¤æ‚æ€§
class AgentState(TypedDict):
    messages: Annotated[list, add_messages]
    current_tool: str
    step_count: int
    intermediate_results: list

# çŠ¶æ€åœ¨å¤šä¸ªèŠ‚ç‚¹é—´ä¼ é€’ï¼Œè°ƒè¯•å¤æ‚
# çŠ¶æ€å†²çªã€ç«æ€æ¡ä»¶ã€å†…å­˜æ³„æ¼ç­‰é—®é¢˜
```

**å­¦ä¹ æ›²çº¿é™¡å³­**ï¼š
- éœ€è¦ç†è§£å›¾è®ºæ¦‚å¿µ
- å¤æ‚çš„çŠ¶æ€æ³¨è§£ç³»ç»Ÿ
- æ£€æŸ¥ç‚¹å’Œæ¢å¤æœºåˆ¶
- æ¡ä»¶è¾¹å’ŒåŠ¨æ€è·¯ç”±

### 3. æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆï¼šå›å½’æœ¬è´¨

#### ç®€æ´ç›´æ¥çš„å®ç°

**é€æ˜çš„æ‰§è¡Œæµç¨‹**ï¼š
```python
# æˆ‘ä»¬çš„æ–¹å¼ï¼šç›´æ¥ã€é€æ˜ã€å¯æ§
llm = LLMProviderFactory.create_provider("openai", config)
messages = [ChatMessage(role="user", content=input_text)]
result = await llm.generate(messages)

# âœ… å®Œå…¨çŸ¥é“æ¯ä¸€æ­¥åœ¨åšä»€ä¹ˆï¼Œé›¶å­¦ä¹ æˆæœ¬
```

**æ˜¾å¼çš„çŠ¶æ€ç®¡ç†**ï¼š
```python
# éœ€è¦çŠ¶æ€ç®¡ç†ï¼Ÿè‡ªå·±æ§åˆ¶ï¼Œä¸éœ€è¦æ¡†æ¶
class ConversationManager:
    def __init__(self):
        self.history: List[ChatMessage] = []
        self.context: Dict[str, Any] = {}
    
    async def chat(self, message: str) -> str:
        # 1. æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        self.history.append(ChatMessage(role="user", content=message))
        
        # 2. ç”Ÿæˆå›å¤
        response = await self.llm.generate(self.history)
        
        # 3. æ›´æ–°å†å²
        self.history.append(ChatMessage(role="assistant", content=response.content))
        
        return response.content

# âœ… ç®€å•ã€å¯æ§ã€æ˜“è°ƒè¯•
```

**å¯ç»„åˆçš„å¤šAgentç³»ç»Ÿ**ï¼š
```python
# å¤šAgentåä½œï¼Ÿç»„åˆæ¨¡å¼è§£å†³
class MultiAgentSystem:
    def __init__(self):
        self.researcher = LLMProviderFactory.create_provider("openai", research_config)
        self.writer = LLMProviderFactory.create_provider("openai", writing_config)
        self.reviewer = LLMProviderFactory.create_provider("anthropic", review_config)
    
    async def collaborative_task(self, task: str) -> str:
        # 1. ç ”ç©¶å‘˜æ”¶é›†ä¿¡æ¯
        research = await self.researcher.generate([
            ChatMessage(role="user", content=f"ç ”ç©¶ä»»åŠ¡: {task}")
        ])
        
        # 2. å†™ä½œå‘˜ç”Ÿæˆå†…å®¹
        content = await self.writer.generate([
            ChatMessage(role="user", content=f"åŸºäºç ”ç©¶ç»“æœå†™ä½œ: {research.content}")
        ])
        
        # 3. å®¡æ ¸å‘˜æ£€æŸ¥è´¨é‡
        review = await self.reviewer.generate([
            ChatMessage(role="user", content=f"å®¡æ ¸å†…å®¹: {content.content}")
        ])
        
        return review.content

# âœ… æ¸…æ™°çš„èŒè´£åˆ†å·¥ï¼Œæ˜“äºç†è§£å’Œè°ƒè¯•
```

## ğŸ”¥ å®é™…åœºæ™¯å¯¹æ¯”ï¼šæ„å»ºRAGç³»ç»Ÿ

### LangChainæ–¹å¼ - é­”æ³•ä½†ä¸å¯æ§

```python
from langchain.chains import RetrievalQA
from langchain.vectorstores import Chroma

# ğŸ¤” éœ€è¦ç†è§£ä¸€å †æ¦‚å¿µï¼šChain, VectorStoreIndex, RetrievalQAç­‰
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",  # ğŸ¤·â€â™‚ï¸ ä»€ä¹ˆæ˜¯"stuff"ï¼Ÿ
    retriever=vectorstore.as_retriever(search_kwargs={"k": 4}),
    return_source_documents=True
)

result = qa_chain({"query": question})  # ğŸ¤·â€â™‚ï¸ å†…éƒ¨åšäº†ä»€ä¹ˆï¼Ÿ
```

### LangGraphæ–¹å¼ - è¿‡åº¦å¤æ‚

```python
from langgraph.graph import StateGraph

# ğŸ¤¯ ä¸ºäº†RAGéœ€è¦å®šä¹‰å¤æ‚çš„å›¾ç»“æ„
class RAGState(TypedDict):
    query: str
    documents: list
    context: str
    response: str

def create_rag_graph():
    workflow = StateGraph(RAGState)
    
    workflow.add_node("retrieve", retrieve_documents)
    workflow.add_node("generate", generate_response)
    workflow.add_edge(START, "retrieve")
    workflow.add_edge("retrieve", "generate")
    workflow.add_edge("generate", END)
    
    return workflow.compile()

# å¤æ‚çš„çŠ¶æ€ä¼ é€’ï¼Œéš¾ä»¥è°ƒè¯•
```

### æˆ‘ä»¬çš„æ–¹å¼ - æ˜¾å¼ä¸”å¯æ§

```python
async def rag_query(question: str):
    # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£ - ä½ å®Œå…¨çŸ¥é“åœ¨åšä»€ä¹ˆ
    search_results = await vector_store.search(question, limit=3)
    
    # 2. æ„å»ºä¸Šä¸‹æ–‡ - ä½ æ§åˆ¶æ ¼å¼
    context = "\n".join([doc.content for doc in search_results.documents])
    
    # 3. ç”Ÿæˆå›ç­” - ä½ è®¾è®¡prompt
    messages = [
        ChatMessage(role="system", content="åŸºäºä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜"),
        ChatMessage(role="user", content=f"æ–‡æ¡£:\n{context}\n\né—®é¢˜: {question}")
    ]
    
    # 4. è¿”å›ç»“æœ - å®Œå…¨é€æ˜
    response = await llm.generate(messages)
    return response.content

# âœ… ç®€å•ã€ç›´æ¥ã€å¯è°ƒè¯•
```

## ğŸ›ï¸ è®¾è®¡å“²å­¦ï¼šUnixæ€æƒ³åœ¨AIæ—¶ä»£çš„åº”ç”¨

æˆ‘ä»¬éµå¾ªLinus Torvaldså’ŒUnixçš„æ ¸å¿ƒå“²å­¦ï¼š

### "Do One Thing and Do It Well"
- **LangChain/LangGraph**: è¯•å›¾ç”¨å¤æ‚æ¡†æ¶è§£å†³æ‰€æœ‰AIåº”ç”¨åœºæ™¯
- **æˆ‘ä»¬**: æ¯ä¸ªProviderä¸“æ³¨ä¸€ä¸ªå…·ä½“åŠŸèƒ½ï¼Œç»„åˆè§£å†³å¤æ‚é—®é¢˜

### "Worse is Better"  
- **LangChain/LangGraph**: è¿½æ±‚"ç†è®ºå®Œç¾"çš„æŠ½è±¡å’Œå¤æ‚çŠ¶æ€ç®¡ç†
- **æˆ‘ä»¬**: è¿½æ±‚"å®é™…å¯ç”¨"çš„ç®€æ´å®ç°

### "Show Me the Code"
- **LangChain/LangGraph**: é»‘ç›’æ‰§è¡Œï¼Œéšè—å®ç°ç»†èŠ‚
- **æˆ‘ä»¬**: é€æ˜å®ç°ï¼Œç”¨æˆ·å®Œå…¨å¯æ§

## âš–ï¸ æŠ€æœ¯å€ºåŠ¡å…¨é¢å¯¹æ¯”

| ç»´åº¦ | LangChain | LangGraph | AI Modular Blocks |
|------|-----------|-----------|-------------------|
| **å­¦ä¹ æˆæœ¬** | é«˜ï¼ˆChainä½“ç³»ï¼‰ | æé«˜ï¼ˆå›¾+çŠ¶æ€ç®¡ç†ï¼‰ | ä½ï¼ˆç®€å•æ¥å£ï¼‰ |
| **è°ƒè¯•éš¾åº¦** | é«˜ï¼ˆé»‘ç›’æ‰§è¡Œï¼‰ | æé«˜ï¼ˆå¤æ‚çŠ¶æ€æµï¼‰ | ä½ï¼ˆé€æ˜å®ç°ï¼‰ |
| **æ€§èƒ½å¼€é”€** | é«˜ï¼ˆå¤šå±‚æŠ½è±¡ï¼‰ | æé«˜ï¼ˆçŠ¶æ€åŒæ­¥ï¼‰ | ä½ï¼ˆç›´æ¥è°ƒç”¨ï¼‰ |
| **ç‰ˆæœ¬é£é™©** | é«˜ï¼ˆbreaking changesï¼‰ | æé«˜ï¼ˆæ–°æ¡†æ¶ä¸ç¨³å®šï¼‰ | ä½ï¼ˆç‹¬ç«‹ç‰ˆæœ¬ï¼‰ |
| **æ‰©å±•æˆæœ¬** | é«˜ï¼ˆå¤æ‚ç»§æ‰¿ï¼‰ | æé«˜ï¼ˆå›¾ç»“æ„ä¿®æ”¹ï¼‰ | ä½ï¼ˆç®€å•æ¥å£ï¼‰ |
| **çŠ¶æ€ç®¡ç†** | éšå¼ï¼ˆéš¾æ§åˆ¶ï¼‰ | æ˜¾å¼ä½†å¤æ‚ | ç”¨æˆ·è‡ªä¸»æ§åˆ¶ |
| **é”™è¯¯å¤„ç†** | å¤æ‚ï¼ˆé“¾å¼ä¼ æ’­ï¼‰ | æå¤æ‚ï¼ˆå›¾ä¸­æ–­æ¢å¤ï¼‰ | ç²¾ç¡®ï¼ˆå¼‚å¸¸å®šä½ï¼‰ |

## ğŸ¯ ä½•æ—¶é€‰æ‹©ä»€ä¹ˆï¼Ÿ

### é€‰æ‹©LangChainçš„åœºæ™¯
- âœ… å¿«é€ŸåŸå‹éªŒè¯ï¼ˆä¸è€ƒè™‘é•¿æœŸç»´æŠ¤ï¼‰
- âœ… å¯¹æ€§èƒ½å’Œå¯æ§æ€§è¦æ±‚ä¸é«˜
- âœ… å›¢é˜Ÿå–œæ¬¢"å¼€ç®±å³ç”¨"çš„è§£å†³æ–¹æ¡ˆ

### é€‰æ‹©LangGraphçš„åœºæ™¯  
- âœ… å¤æ‚çš„å¤šAgentåä½œï¼ˆæœ‰å……è¶³çš„å­¦ä¹ æ—¶é—´ï¼‰
- âœ… éœ€è¦å¤æ‚çŠ¶æ€ç®¡ç†ï¼ˆå›¢é˜Ÿæœ‰å›¾è®ºèƒŒæ™¯ï¼‰
- âœ… ç ”ç©¶å’Œå®éªŒé¡¹ç›®ï¼ˆä¸æ˜¯ç”Ÿäº§ç¯å¢ƒï¼‰

### é€‰æ‹©AI Modular Blocksçš„åœºæ™¯
- âœ… **ç”Ÿäº§ç¯å¢ƒåº”ç”¨**
- âœ… **éœ€è¦ç²¾ç¡®æ§åˆ¶å’Œæ€§èƒ½ä¼˜åŒ–**
- âœ… **é•¿æœŸç»´æŠ¤å’Œæ¼”è¿›çš„é¡¹ç›®**
- âœ… **å›¢é˜Ÿæƒ³è¦å®Œå…¨ç†è§£ç³»ç»Ÿè¡Œä¸º**
- âœ… **å¿«é€Ÿå¼€å‘å’Œè¿­ä»£**

> **Linusè¯´**: "Theory and practice sometimes clash. Theory loses. Every single time."
> 
> LangChainå’ŒLangGraphéƒ½æ˜¯ç†è®ºä¸Šçš„"å®Œç¾"ï¼Œæˆ‘ä»¬æ˜¯å®è·µä¸­çš„"å¯ç”¨"ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
# åŸºç¡€å®‰è£…
pip install ai-modular-blocks

# å¸¦ç‰¹å®šprovideræ”¯æŒ
pip install ai-modular-blocks[openai,chroma]  # LLM + å‘é‡å­˜å‚¨
pip install ai-modular-blocks[all]           # å…¨éƒ¨åŠŸèƒ½
```

### 30ç§’è·‘èµ·ç¬¬ä¸€ä¸ªç¤ºä¾‹

```python
import asyncio
import os
from ai_modular_blocks.providers.llm import LLMProviderFactory
from ai_modular_blocks.core.types import LLMConfig, ChatMessage

async def hello_ai():
    # 1. åˆ›å»ºLLM provider
    config = LLMConfig(
        provider="openai",
        model="gpt-3.5-turbo", 
        api_key=os.getenv("OPENAI_API_KEY")
    )
    
    LLMProviderFactory.initialize()
    llm = LLMProviderFactory.create_provider("openai", config)
    
    # 2. å‘é€æ¶ˆæ¯
    messages = [ChatMessage(role="user", content="Hello, AI!")]
    response = await llm.generate(messages)
    
    print(f"AIå›å¤: {response.content}")

# è¿è¡Œ
asyncio.run(hello_ai())
```

### å®é™…åº”ç”¨ï¼šæ„å»ºRAGç³»ç»Ÿ

```python
from ai_modular_blocks.providers.llm import LLMProviderFactory
from ai_modular_blocks.providers.vectorstores import VectorStoreFactory
from ai_modular_blocks.core.types import VectorDocument

async def build_rag_system():
    # åˆå§‹åŒ–ç»„ä»¶
    LLMProviderFactory.initialize()
    VectorStoreFactory.initialize()
    
    # LLM for generation
    llm = LLMProviderFactory.create_provider("openai", llm_config)
    
    # Vector store for retrieval
    vector_store = VectorStoreFactory.create_provider("chroma", vector_config)
    
    # æ·»åŠ æ–‡æ¡£
    docs = [
        VectorDocument(id="1", content="Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€...", metadata={"topic": "programming"}),
        VectorDocument(id="2", content="æœºå™¨å­¦ä¹ æ˜¯AIçš„åˆ†æ”¯...", metadata={"topic": "ai"})
    ]
    await vector_store.add_documents(docs)
    
    # RAGæŸ¥è¯¢
    async def rag_query(question: str):
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        results = await vector_store.search(question, limit=3)
        context = "\n".join([doc.content for doc in results.documents])
        
        # 2. ç”Ÿæˆå›ç­”
        messages = [
            ChatMessage(role="system", content="åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜"),
            ChatMessage(role="user", content=f"ä¸Šä¸‹æ–‡: {context}\n\né—®é¢˜: {question}")
        ]
        response = await llm.generate(messages)
        return response.content
    
    # ä½¿ç”¨
    answer = await rag_query("ä»€ä¹ˆæ˜¯Pythonï¼Ÿ")
    print(answer)
```

æ›´å¤šç¤ºä¾‹è§ [`examples/`](../examples/) ç›®å½•ã€‚

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ ¸å¿ƒæ¶æ„

```mermaid
graph TB
    subgraph "ç”¨æˆ·åº”ç”¨å±‚"
        A1[RAGåº”ç”¨] 
        A2[èŠå¤©åº”ç”¨]
        A3[å¤šAgentç³»ç»Ÿ]
    end
    
    subgraph "åè°ƒå±‚"
        B1[Factoryæ¨¡å¼]
        B2[é…ç½®ç®¡ç†]
        B3[é”™è¯¯å¤„ç†]
    end
    
    subgraph "æŠ½è±¡å±‚"
        C1[LLMProvider]
        C2[VectorStore]
        C3[EmbeddingProvider]
    end
    
    subgraph "å®ç°å±‚"
        D1[OpenAI]
        D2[Anthropic]
        D3[Chroma]
        D4[Pinecone]
    end
    
    A1 --> B1
    A2 --> B1
    A3 --> B1
    B1 --> C1
    B1 --> C2
    B1 --> C3
    C1 --> D1
    C1 --> D2
    C2 --> D3
    C2 --> D4
```

### é¡¹ç›®ç»“æ„

```
ai_modular_blocks/
â”œâ”€â”€ core/                      # ğŸ›ï¸ æ ¸å¿ƒæŠ½è±¡å±‚
â”‚   â”œâ”€â”€ interfaces.py         # æ ‡å‡†æ¥å£å®šä¹‰
â”‚   â”œâ”€â”€ types.py              # æ•°æ®ç±»å‹
â”‚   â”œâ”€â”€ exceptions.py         # å¼‚å¸¸ä½“ç³»
â”‚   â”œâ”€â”€ config.py             # é…ç½®ç®¡ç†
â”‚   â””â”€â”€ base.py               # åŸºç¡€å®ç°ç±»
â”œâ”€â”€ providers/                 # ğŸ”Œ å…·ä½“å®ç°å±‚
â”‚   â”œâ”€â”€ llm/                  # LLMæä¾›å•†
â”‚   â”‚   â”œâ”€â”€ openai_provider.py
â”‚   â”‚   â”œâ”€â”€ anthropic_provider.py
â”‚   â”‚   â””â”€â”€ factory.py
â”‚   â”œâ”€â”€ vectorstores/         # å‘é‡å­˜å‚¨
â”‚   â”‚   â”œâ”€â”€ chroma_store.py
â”‚   â”‚   â”œâ”€â”€ pinecone_store.py
â”‚   â”‚   â””â”€â”€ factory.py
â”‚   â””â”€â”€ embeddings/           # åµŒå…¥æ¨¡å‹
â””â”€â”€ utils/                     # ğŸ› ï¸ å·¥å…·å±‚
    â”œâ”€â”€ caching/              # ç¼“å­˜ç­–ç•¥
    â”œâ”€â”€ monitoring/           # æ€§èƒ½ç›‘æ§
    â””â”€â”€ logging/              # æ—¥å¿—ç®¡ç†
```

## ğŸ”Œ æ ¸å¿ƒæ¥å£

### LLM Provideræ¥å£

```python
class LLMProvider(ABC):
    """LLMæä¾›å•†æ ‡å‡†æ¥å£"""
    
    @abstractmethod
    async def generate(
        self,
        messages: MessageList,
        **kwargs
    ) -> LLMResponse:
        """ç”Ÿæˆå›å¤"""
        pass
    
    @abstractmethod
    async def stream_generate(
        self,
        messages: MessageList,
        **kwargs
    ) -> AsyncGenerator[LLMResponse, None]:
        """æµå¼ç”Ÿæˆ"""
        pass
```

### Vector Storeæ¥å£

```python
class VectorStore(ABC):
    """å‘é‡å­˜å‚¨æ ‡å‡†æ¥å£"""
    
    @abstractmethod
    async def add_documents(
        self,
        documents: DocumentList,
        **kwargs
    ) -> Dict[str, Any]:
        """æ·»åŠ æ–‡æ¡£"""
        pass
    
    @abstractmethod
    async def search(
        self,
        query: str,
        limit: int = 5,
        **kwargs
    ) -> SearchResult:
        """æœç´¢ç›¸ä¼¼æ–‡æ¡£"""
        pass
```

## ğŸ“Š æ”¯æŒçš„Provider

### LLMæä¾›å•†

| Provider | æ¨¡å‹æ”¯æŒ | çŠ¶æ€ | ç‰¹æ€§ |
|----------|---------|------|------|
| **OpenAI** | GPT-3.5, GPT-4, GPT-4o | âœ… ç”Ÿäº§å°±ç»ª | å®Œæ•´APIæ”¯æŒï¼Œæµå¼å“åº” |
| **Anthropic** | Claude-3 Haiku, Sonnet, Opus | âœ… ç”Ÿäº§å°±ç»ª | é•¿ä¸Šä¸‹æ–‡ï¼Œå®‰å…¨å¯¹é½ |
| **Local Models** | Ollama, vLLM | ğŸš§ å¼€å‘ä¸­ | æœ¬åœ°éƒ¨ç½²ï¼Œéšç§ä¿æŠ¤ |

### å‘é‡å­˜å‚¨

| Provider | éƒ¨ç½²æ–¹å¼ | çŠ¶æ€ | ç‰¹æ€§ |
|----------|---------|------|------|
| **ChromaDB** | æœ¬åœ°/Docker | âœ… ç”Ÿäº§å°±ç»ª | å…è´¹ï¼Œæ˜“éƒ¨ç½² |
| **Pinecone** | äº‘æœåŠ¡ | âœ… ç”Ÿäº§å°±ç»ª | é«˜æ€§èƒ½ï¼Œè‡ªåŠ¨æ‰©å±• |
| **Weaviate** | è‡ªæ‰˜ç®¡ | ğŸš§ å¼€å‘ä¸­ | å¼€æºï¼Œå›¾æ•°æ®åº“ |

## âš¡ æ€§èƒ½ç‰¹æ€§

### è¿æ¥æ± ç®¡ç†
- æ‰€æœ‰HTTPå®¢æˆ·ç«¯ä½¿ç”¨è¿æ¥æ± 
- è‡ªåŠ¨é‡è¯•å’ŒæŒ‡æ•°é€€é¿
- è¶…æ—¶å’Œé€Ÿç‡é™åˆ¶å¤„ç†

### ç¼“å­˜ç­–ç•¥
```python
# è‡ªåŠ¨ç¼“å­˜LLMå“åº”
@cache_result(ttl=3600, cache_type="memory")
async def cached_llm_call(messages):
    return await llm.generate(messages)

# ç¼“å­˜å‘é‡æŸ¥è¯¢ç»“æœ
@cache_result(ttl=1800, cache_type="redis")
async def cached_vector_search(query):
    return await vector_store.search(query)
```

### ç›‘æ§æŒ‡æ ‡
```python
# å†…ç½®PrometheusæŒ‡æ ‡
llm_requests_total         # LLMè¯·æ±‚è®¡æ•°
llm_request_duration_seconds  # LLMè¯·æ±‚å»¶è¿Ÿ
vector_search_total        # å‘é‡æœç´¢è®¡æ•°
cache_hit_rate            # ç¼“å­˜å‘½ä¸­ç‡
```

## ğŸ§ª æŠ€æœ¯è§„èŒƒ

### é”™è¯¯å¤„ç†

```python
# ç»Ÿä¸€å¼‚å¸¸ä½“ç³»
AIBlocksException          # åŸºç¡€å¼‚å¸¸
â”œâ”€â”€ ProviderException      # æä¾›å•†å¼‚å¸¸
â”‚   â”œâ”€â”€ AuthenticationException  # è®¤è¯å¤±è´¥
â”‚   â”œâ”€â”€ RateLimitException      # é€Ÿç‡é™åˆ¶
â”‚   â””â”€â”€ TimeoutException        # è¯·æ±‚è¶…æ—¶
â”œâ”€â”€ ConfigurationException # é…ç½®é”™è¯¯
â””â”€â”€ ValidationException    # è¾“å…¥éªŒè¯å¤±è´¥

# è‡ªåŠ¨é‡è¯•
@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    retry=retry_if_exception_type((TimeoutException, RateLimitException))
)
async def robust_llm_call():
    return await llm.generate(messages)
```

### é…ç½®ç®¡ç†

```python
# ç¯å¢ƒé…ç½®
OPENAI_API_KEY=your-openai-key
ANTHROPIC_API_KEY=your-anthropic-key
PINECONE_API_KEY=your-pinecone-key

# ä»£ç é…ç½®
config = LLMConfig(
    provider="openai",
    model="gpt-4",
    api_key=os.getenv("OPENAI_API_KEY"),
    max_tokens=1000,
    temperature=0.7,
    timeout=30.0,
    max_retries=3
)
```

### æµ‹è¯•ç­–ç•¥

```bash
# è¿è¡Œæµ‹è¯•å¥—ä»¶
pytest tests/ -v --cov=ai_modular_blocks

# æµ‹è¯•è¦†ç›–ç‡ç›®æ ‡
# å•å…ƒæµ‹è¯•: >90%
# é›†æˆæµ‹è¯•: æ ¸å¿ƒåŠŸèƒ½100%
# æ€§èƒ½æµ‹è¯•: P99 < 500ms
```

## ğŸ”§ å¼€å‘æŒ‡å—

### æ·»åŠ æ–°çš„Provider

1. **å®ç°æ¥å£**
```python
class NewLLMProvider(BaseLLMProvider):
    async def generate(self, messages: MessageList, **kwargs) -> LLMResponse:
        # å®ç°å…·ä½“é€»è¾‘
        pass
```

2. **æ³¨å†Œåˆ°å·¥å‚**
```python
# åœ¨å·¥å‚ç±»ä¸­æ³¨å†Œ
LLMProviderFactory.register_provider("new_provider", NewLLMProvider)
```

3. **æ·»åŠ æµ‹è¯•**
```python
class TestNewProvider:
    async def test_generate(self):
        provider = NewLLMProvider(config)
        response = await provider.generate(messages)
        assert response.content
```

### è´¡çŒ®ä»£ç 

```bash
# 1. å®‰è£…å¼€å‘ä¾èµ–
pip install -e ".[dev]"

# 2. ä»£ç è´¨é‡æ£€æŸ¥
ruff check .           # ä»£ç æ£€æŸ¥ï¼ˆæ›¿ä»£ flake8 + isortï¼‰
ruff format .          # ä»£ç æ ¼å¼åŒ–ï¼ˆæ›¿ä»£ blackï¼‰
mypy .                 # ç±»å‹æ£€æŸ¥

# æˆ–è€…ä¸€é”®ä¿®å¤æ‰€æœ‰å¯ä¿®å¤é—®é¢˜
ruff check --fix .     # è‡ªåŠ¨ä¿®å¤ä»£ç é—®é¢˜

# 3. è¿è¡Œæµ‹è¯•
pytest tests/ --cov=ai_modular_blocks

# 4. æ¸…ç†Pythonç¼“å­˜ï¼ˆå¦‚æœæ„å¤–æäº¤äº†ï¼‰
find . -name "__pycache__" -type d -not -path "./.venv/*" -exec git rm -r --cached {} \; 2>/dev/null || true
```

**âš ï¸ æ³¨æ„**: é¡¹ç›®å·²æ­£ç¡®é…ç½®`.gitignore`æ¥å¿½ç•¥æ‰€æœ‰`__pycache__`æ–‡ä»¶å¤¹ã€‚å¦‚æœä½ çœ‹åˆ°è¿™äº›æ–‡ä»¶è¢«gitè·Ÿè¸ªï¼Œè¯´æ˜å®ƒä»¬åœ¨é…ç½®å‰å°±è¢«æ·»åŠ äº†ï¼Œéœ€è¦æ‰‹åŠ¨æ¸…ç†ã€‚

### æ‰©å±•æ–°åŠŸèƒ½

æƒ³è¦æ·»åŠ æ–°çš„æŠ½è±¡å±‚ï¼Ÿ
1. åœ¨`core/interfaces.py`å®šä¹‰æ¥å£
2. åœ¨`core/types.py`æ·»åŠ æ•°æ®ç±»å‹
3. åœ¨`providers/`åˆ›å»ºå…·ä½“å®ç°
4. åœ¨`tests/`æ·»åŠ æµ‹è¯•ç”¨ä¾‹

## ğŸ† æœ€ä½³å®è·µ

### 1. æ¥å£ä¼˜äºå®ç°
```python
# âœ… å¥½çš„æ–¹å¼ - ä¾èµ–æŠ½è±¡
llm: LLMProvider = factory.create_provider("openai", config)

# âŒ åçš„æ–¹å¼ - ä¾èµ–å…·ä½“å®ç°
llm = OpenAIProvider(config)
```

### 2. é…ç½®å¤–éƒ¨åŒ–
```python
# âœ… å¥½çš„æ–¹å¼ - é…ç½®ä¸ä»£ç åˆ†ç¦»
config = load_config_from_env()
llm = factory.create_provider("openai", config)

# âŒ åçš„æ–¹å¼ - ç¡¬ç¼–ç é…ç½®
llm = OpenAIProvider(api_key="sk-hardcoded-key")
```

### 3. é”™è¯¯å¤„ç†è¦å½»åº•
```python
# âœ… å¥½çš„æ–¹å¼ - å¤„ç†å…·ä½“å¼‚å¸¸
try:
    response = await llm.generate(messages)
except RateLimitException:
    await asyncio.sleep(60)  # ç­‰å¾…åé‡è¯•
except AuthenticationException:
    logger.error("API key invalid")
    raise
```

### 4. ä½¿ç”¨å·¥å‚æ¨¡å¼
```python
# âœ… å¥½çš„æ–¹å¼ - å·¥å‚ç»Ÿä¸€ç®¡ç†
LLMProviderFactory.initialize()
llm = LLMProviderFactory.create_provider("openai", config)

# âŒ åçš„æ–¹å¼ - ç›´æ¥å®ä¾‹åŒ–
llm = OpenAIProvider(config)
```

## ğŸ“ˆ æ€§èƒ½åŸºå‡†

### å»¶è¿Ÿç›®æ ‡

| æ“ä½œ | P50 | P99 | ç›®æ ‡ |
|------|-----|-----|------|
| LLMç”Ÿæˆ | <2s | <5s | âœ… |
| å‘é‡æœç´¢ | <100ms | <500ms | âœ… |
| ç¼“å­˜å‘½ä¸­ | <5ms | <10ms | âœ… |

### ååé‡

| Provider | å¹¶å‘æ•° | QPS | çŠ¶æ€ |
|----------|--------|-----|------|
| OpenAI | 50 | 100 | âœ… |
| Anthropic | 30 | 60 | âœ… |
| ChromaDB | 100 | 1000 | âœ… |

## ğŸ“ æ›´æ–°æ—¥å¿—

### v0.1.0 (å½“å‰)
- âœ… æ ¸å¿ƒLLM providers (OpenAI, Anthropic)
- âœ… å‘é‡å­˜å‚¨ (ChromaDB, Pinecone)
- âœ… å®Œæ•´çš„ç±»å‹ç³»ç»Ÿå’Œå¼‚å¸¸å¤„ç†
- âœ… è‡ªåŠ¨é‡è¯•å’Œç¼“å­˜
- âœ… 90%+ æµ‹è¯•è¦†ç›–ç‡

### è·¯çº¿å›¾
- ğŸš§ æ›´å¤šembedding providers
- ğŸš§ æµå¼å¤„ç†ä¼˜åŒ–
- ğŸš§ åˆ†å¸ƒå¼å‘é‡å­˜å‚¨
- ğŸš§ å¯è§†åŒ–ç›‘æ§é¢æ¿

## ğŸ“Œ å¼€å‘è®¡åˆ’ / TODO

- è¯¦ç»†çš„ä¼˜åŒ–è®¡åˆ’ä¸æ‰§è¡Œæ¸…å•è¯·è§ï¼š`docs/TODO.md`
- è¦†ç›–ç¨³å®šæ€§ã€å¯è§‚æµ‹æ€§ã€ç¤ºä¾‹ä¸€è‡´æ€§ã€å®‰å…¨æ€§ã€CI ä¸æ–‡æ¡£ç­‰æ–¹å‘ã€‚

## ğŸ†˜ è·å–å¸®åŠ©

- ğŸ“– **æ–‡æ¡£**: ä½ æ­£åœ¨çœ‹çš„å°±æ˜¯
- ğŸ› **BugæŠ¥å‘Š**: [GitHub Issues](https://github.com/your-username/ai-modular-blocks/issues)
- ğŸ’¬ **è®¨è®º**: [GitHub Discussions](https://github.com/your-username/ai-modular-blocks/discussions)
- ğŸ“§ **é‚®ä»¶**: your-email@example.com

## ğŸ“„ è®¸å¯è¯

MIT License - è‡ªç”±ä½¿ç”¨ã€ä¿®æ”¹å’Œåˆ†å‘ã€‚

---

**ğŸ¯ Linusè¯´**: "Talk is cheap. Show me the code." 

ä¸è¦å…‰çœ‹æ–‡æ¡£ï¼Œç›´æ¥è¿è¡Œ `examples/` é‡Œçš„ä»£ç ï¼Œæ¯”çœ‹ä¸€åƒé¡µæ–‡æ¡£æ›´æœ‰ç”¨ï¼
