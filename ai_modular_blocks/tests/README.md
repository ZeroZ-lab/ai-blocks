# Tests Module

æµ‹è¯•æ¨¡å— - AI Modular Blocksçš„è´¨é‡ä¿è¯å±‚

## ğŸ¯ æµ‹è¯•ç­–ç•¥

éµå¾ª"**æµ‹è¯•é‡‘å­—å¡”**"åŸåˆ™ï¼Œç¡®ä¿ä»£ç è´¨é‡å’Œç³»ç»Ÿå¯é æ€§ï¼š

```
    ğŸ”º E2E Tests (å°‘é‡)
   ğŸ”ºğŸ”º Integration Tests (é€‚é‡)  
  ğŸ”ºğŸ”ºğŸ”º Unit Tests (å¤§é‡)
```

## ğŸ“ ç›®å½•ç»„ç»‡

### `unit/` - å•å…ƒæµ‹è¯•
æµ‹è¯•å•ä¸ªæ¨¡å—ã€ç±»å’Œå‡½æ•°çš„åŠŸèƒ½ï¼š

```
unit/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ test_types.py         # ç±»å‹å®šä¹‰æµ‹è¯•
â”‚   â”œâ”€â”€ test_interfaces.py    # æ¥å£å®šä¹‰æµ‹è¯•
â”‚   â”œâ”€â”€ test_base.py          # åŸºç¡€ç±»æµ‹è¯•
â”‚   â”œâ”€â”€ test_config.py        # é…ç½®ç®¡ç†æµ‹è¯•
â”‚   â”œâ”€â”€ test_exceptions.py    # å¼‚å¸¸å¤„ç†æµ‹è¯•
â”‚   â””â”€â”€ test_validators.py    # éªŒè¯å™¨æµ‹è¯•
â”œâ”€â”€ providers/
â”‚   â”œâ”€â”€ test_llm_providers.py # LLMæä¾›å•†æµ‹è¯•
â”‚   â”œâ”€â”€ test_vector_stores.py # å‘é‡å­˜å‚¨æµ‹è¯•
â”‚   â”œâ”€â”€ test_embeddings.py    # åµŒå…¥æœåŠ¡æµ‹è¯•
â”‚   â””â”€â”€ test_tools.py         # å·¥å…·æä¾›å•†æµ‹è¯•
â”œâ”€â”€ processors/
â”‚   â”œâ”€â”€ test_loaders.py       # æ–‡æ¡£åŠ è½½å™¨æµ‹è¯•
â”‚   â”œâ”€â”€ test_splitters.py     # æ–‡æ¡£åˆ†å‰²å™¨æµ‹è¯•
â”‚   â””â”€â”€ test_transformers.py  # æ–‡æ¡£è½¬æ¢å™¨æµ‹è¯•
â””â”€â”€ utils/
    â”œâ”€â”€ test_caching.py       # ç¼“å­˜ç³»ç»Ÿæµ‹è¯•
    â”œâ”€â”€ test_logging.py       # æ—¥å¿—ç³»ç»Ÿæµ‹è¯•
    â””â”€â”€ test_monitoring.py    # ç›‘æ§ç³»ç»Ÿæµ‹è¯•
```

**ç‰¹ç‚¹**: å¿«é€Ÿæ‰§è¡Œã€æ— å¤–éƒ¨ä¾èµ–ã€é«˜è¦†ç›–ç‡

### `integration/` - é›†æˆæµ‹è¯•
æµ‹è¯•ç»„ä»¶é—´çš„äº¤äº’å’Œä¸å¤–éƒ¨æœåŠ¡çš„é›†æˆï¼š

```
integration/
â”œâ”€â”€ test_llm_integration.py    # LLMæœåŠ¡é›†æˆæµ‹è¯•
â”œâ”€â”€ test_vector_integration.py # å‘é‡æ•°æ®åº“é›†æˆæµ‹è¯•
â”œâ”€â”€ test_cache_integration.py  # ç¼“å­˜æœåŠ¡é›†æˆæµ‹è¯•
â”œâ”€â”€ test_rag_pipeline.py       # RAGæµæ°´çº¿é›†æˆæµ‹è¯•
â”œâ”€â”€ test_tool_execution.py     # å·¥å…·æ‰§è¡Œé›†æˆæµ‹è¯•
â””â”€â”€ test_mcp_protocol.py       # MCPåè®®é›†æˆæµ‹è¯•
```

**ç‰¹ç‚¹**: çœŸå®ç¯å¢ƒã€å¤–éƒ¨ä¾èµ–ã€ç«¯åˆ°ç«¯åœºæ™¯

### `performance/` - æ€§èƒ½æµ‹è¯•
æµ‹è¯•ç³»ç»Ÿçš„æ€§èƒ½ã€å¹¶å‘å’Œè´Ÿè½½èƒ½åŠ›ï¼š

```
performance/
â”œâ”€â”€ test_llm_throughput.py     # LLMååé‡æµ‹è¯•
â”œâ”€â”€ test_vector_search.py      # å‘é‡æœç´¢æ€§èƒ½æµ‹è¯•
â”œâ”€â”€ test_concurrent_requests.py # å¹¶å‘è¯·æ±‚æµ‹è¯•
â”œâ”€â”€ test_memory_usage.py       # å†…å­˜ä½¿ç”¨æµ‹è¯•
â”œâ”€â”€ test_cache_performance.py  # ç¼“å­˜æ€§èƒ½æµ‹è¯•
â””â”€â”€ benchmarks/
    â”œâ”€â”€ llm_benchmark.py       # LLMæ€§èƒ½åŸºå‡†
    â”œâ”€â”€ vector_benchmark.py    # å‘é‡æ“ä½œåŸºå‡†
    â””â”€â”€ end_to_end_benchmark.py # ç«¯åˆ°ç«¯æ€§èƒ½åŸºå‡†
```

**ç‰¹ç‚¹**: æ€§èƒ½æŒ‡æ ‡ã€èµ„æºç›‘æ§ã€åŸºå‡†å¯¹æ¯”

## ğŸ§ª æµ‹è¯•å·¥å…·å’Œæ¡†æ¶

### æ ¸å¿ƒæµ‹è¯•æ¡†æ¶
```python
import pytest
import asyncio
from unittest.mock import Mock, AsyncMock, patch
from ai_modular_blocks.tests.fixtures import *
```

### å¸¸ç”¨æµ‹è¯•è£…é¥°å™¨
```python
@pytest.mark.asyncio      # å¼‚æ­¥æµ‹è¯•
@pytest.mark.unit         # å•å…ƒæµ‹è¯•æ ‡è®°
@pytest.mark.integration  # é›†æˆæµ‹è¯•æ ‡è®°
@pytest.mark.performance  # æ€§èƒ½æµ‹è¯•æ ‡è®°
@pytest.mark.slow         # æ…¢é€Ÿæµ‹è¯•æ ‡è®°
@pytest.mark.external     # éœ€è¦å¤–éƒ¨æœåŠ¡çš„æµ‹è¯•
```

### Mockå’ŒFixture
```python
# tests/fixtures/llm_fixtures.py
import pytest
from unittest.mock import AsyncMock
from ai_modular_blocks.core.types import LLMResponse

@pytest.fixture
def mock_llm_provider():
    """Mock LLM Provider fixture"""
    provider = AsyncMock()
    provider.chat_completion.return_value = LLMResponse(
        content="Test response",
        model="test-model",
        usage={"prompt_tokens": 10, "completion_tokens": 5},
        created_at=datetime.now(),
        finish_reason="stop"
    )
    return provider

@pytest.fixture
async def real_openai_provider():
    """çœŸå®çš„OpenAI Providerï¼ˆéœ€è¦APIå¯†é’¥ï¼‰"""
    if not os.getenv("OPENAI_API_KEY"):
        pytest.skip("éœ€è¦OPENAI_API_KEYç¯å¢ƒå˜é‡")
    
    config = LLMConfig(api_key=os.getenv("OPENAI_API_KEY"))
    provider = OpenAIProvider(config)
    await provider.initialize()
    yield provider
    await provider.cleanup()
```

## ğŸ“– æµ‹è¯•ç¤ºä¾‹

### å•å…ƒæµ‹è¯•ç¤ºä¾‹
```python
# tests/unit/core/test_validators.py
import pytest
from ai_modular_blocks.core.validators import InputValidator
from ai_modular_blocks.core.exceptions import ValidationException
from ai_modular_blocks.core.types import ChatMessage

class TestInputValidator:
    
    def test_validate_api_key_valid(self):
        """æµ‹è¯•æœ‰æ•ˆAPIå¯†é’¥éªŒè¯"""
        api_key = "sk-test1234567890abcdef1234567890abcdef123456"
        result = InputValidator.validate_api_key(api_key, "openai")
        assert result == api_key
    
    def test_validate_api_key_invalid_format(self):
        """æµ‹è¯•æ— æ•ˆAPIå¯†é’¥æ ¼å¼"""
        with pytest.raises(ValidationException) as exc_info:
            InputValidator.validate_api_key("invalid-key", "openai")
        
        assert "Invalid API key format" in str(exc_info.value)
    
    def test_validate_chat_message_valid(self):
        """æµ‹è¯•æœ‰æ•ˆèŠå¤©æ¶ˆæ¯éªŒè¯"""
        message = ChatMessage(role="user", content="Hello")
        result = InputValidator.validate_chat_message(message)
        assert result == message
    
    @pytest.mark.parametrize("role", ["invalid", "bot", ""])
    def test_validate_chat_message_invalid_role(self, role):
        """æµ‹è¯•æ— æ•ˆè§’è‰²éªŒè¯"""
        message = ChatMessage(role=role, content="Hello")
        with pytest.raises(ValidationException):
            InputValidator.validate_chat_message(message)
```

### é›†æˆæµ‹è¯•ç¤ºä¾‹
```python
# tests/integration/test_rag_pipeline.py
import pytest
from ai_modular_blocks.core.types import ChatMessage, VectorDocument
from ai_modular_blocks.providers.llm import OpenAIProvider
from ai_modular_blocks.providers.vectorstores import ChromaProvider
from ai_modular_blocks.providers.embeddings import OpenAIEmbeddingProvider

@pytest.mark.integration
@pytest.mark.asyncio
async def test_rag_pipeline_end_to_end(
    openai_config,
    chroma_config,
    sample_documents
):
    """æµ‹è¯•å®Œæ•´çš„RAGæµæ°´çº¿"""
    
    # åˆå§‹åŒ–ç»„ä»¶
    llm = OpenAIProvider(openai_config)
    vector_store = ChromaProvider(chroma_config)
    embedding = OpenAIEmbeddingProvider(openai_config)
    
    await llm.initialize()
    await vector_store.initialize()
    await embedding.initialize()
    
    try:
        # 1. åµŒå…¥å’Œå­˜å‚¨æ–‡æ¡£
        embedded_docs = await embedding.embed_documents(sample_documents)
        upsert_result = await vector_store.upsert(embedded_docs)
        assert upsert_result["processed_count"] == len(sample_documents)
        
        # 2. æœç´¢ç›¸å…³æ–‡æ¡£
        query = "What is the capital of France?"
        query_embedding = await embedding.embed_text([query])
        search_results = await vector_store.search(
            query_embedding[0], 
            top_k=3
        )
        assert len(search_results) > 0
        
        # 3. ç”Ÿæˆå›ç­”
        context = "\n".join([result.document.content for result in search_results])
        messages = [
            ChatMessage(role="system", content=f"Context: {context}"),
            ChatMessage(role="user", content=query)
        ]
        
        response = await llm.chat_completion(messages, model="gpt-3.5-turbo")
        assert response.content
        assert len(response.content) > 0
        
    finally:
        await llm.cleanup()
        await vector_store.cleanup()
        await embedding.cleanup()
```

### æ€§èƒ½æµ‹è¯•ç¤ºä¾‹
```python
# tests/performance/test_llm_throughput.py
import asyncio
import time
import pytest
from ai_modular_blocks.providers.llm import OpenAIProvider

@pytest.mark.performance
@pytest.mark.asyncio
async def test_llm_concurrent_requests(openai_provider):
    """æµ‹è¯•LLMå¹¶å‘è¯·æ±‚æ€§èƒ½"""
    
    messages = [ChatMessage(role="user", content="Say hello")]
    
    async def single_request():
        start_time = time.time()
        response = await openai_provider.chat_completion(
            messages, 
            model="gpt-3.5-turbo"
        )
        end_time = time.time()
        return {
            "duration": end_time - start_time,
            "response_length": len(response.content),
            "tokens_used": response.usage.get("total_tokens", 0)
        }
    
    # å¹¶å‘æ‰§è¡Œå¤šä¸ªè¯·æ±‚
    concurrent_requests = 10
    start_time = time.time()
    
    tasks = [single_request() for _ in range(concurrent_requests)]
    results = await asyncio.gather(*tasks)
    
    total_time = time.time() - start_time
    
    # æ€§èƒ½æ–­è¨€
    assert total_time < 30  # 10ä¸ªè¯·æ±‚åœ¨30ç§’å†…å®Œæˆ
    
    avg_duration = sum(r["duration"] for r in results) / len(results)
    assert avg_duration < 5  # å¹³å‡å“åº”æ—¶é—´å°äº5ç§’
    
    # è®°å½•æ€§èƒ½æŒ‡æ ‡
    print(f"å¹¶å‘è¯·æ±‚æ•°: {concurrent_requests}")
    print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"å¹³å‡å“åº”æ—¶é—´: {avg_duration:.2f}ç§’")
    print(f"QPS: {concurrent_requests / total_time:.2f}")
```

## ğŸš€ è¿è¡Œæµ‹è¯•

### åŸºæœ¬æµ‹è¯•å‘½ä»¤
```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest

# è¿è¡Œç‰¹å®šç±»å‹çš„æµ‹è¯•
pytest -m unit           # åªè¿è¡Œå•å…ƒæµ‹è¯•
pytest -m integration    # åªè¿è¡Œé›†æˆæµ‹è¯•
pytest -m performance    # åªè¿è¡Œæ€§èƒ½æµ‹è¯•

# è¿è¡Œç‰¹å®šæ–‡ä»¶æˆ–ç›®å½•
pytest tests/unit/core/
pytest tests/integration/test_rag_pipeline.py

# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest --cov=ai_modular_blocks --cov-report=html

# è¯¦ç»†è¾“å‡º
pytest -v -s

# å¹¶è¡Œæ‰§è¡Œï¼ˆéœ€è¦pytest-xdistï¼‰
pytest -n auto
```

### ç¯å¢ƒé…ç½®
```bash
# è®¾ç½®æµ‹è¯•ç¯å¢ƒå˜é‡
export OPENAI_API_KEY="your-api-key"
export PINECONE_API_KEY="your-pinecone-key"
export TEST_ENV="testing"

# æˆ–ä½¿ç”¨ .env.test æ–‡ä»¶
cp .env.example .env.test
# ç¼–è¾‘ .env.test æ–‡ä»¶
```

### Dockeræµ‹è¯•ç¯å¢ƒ
```yaml
# docker-compose.test.yml
version: '3.8'
services:
  test-runner:
    build: .
    environment:
      - TEST_ENV=docker
    volumes:
      - .:/app
    command: pytest tests/
  
  redis-test:
    image: redis:alpine
    ports:
      - "6380:6379"
  
  chroma-test:
    image: chromadb/chroma
    ports:
      - "8001:8000"
```

## ğŸ”§ æµ‹è¯•é…ç½®

### pytest.inié…ç½®
```ini
[tool:pytest]
minversion = 6.0
addopts = 
    -ra 
    --strict-markers 
    --strict-config 
    --cov=ai_modular_blocks
    --cov-branch
    --cov-report=term-missing
    --cov-fail-under=80

testpaths = tests

markers =
    unit: Unit tests
    integration: Integration tests  
    performance: Performance tests
    slow: Slow running tests
    external: Tests requiring external services

asyncio_mode = auto
```

### conftest.pyé…ç½®
```python
# tests/conftest.py
import pytest
import asyncio
import os
from unittest.mock import AsyncMock

# å…¨å±€æµ‹è¯•é…ç½®
@pytest.fixture(scope="session")
def event_loop():
    """åˆ›å»ºäº‹ä»¶å¾ªç¯ä¾›æ•´ä¸ªæµ‹è¯•ä¼šè¯ä½¿ç”¨"""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

@pytest.fixture(autouse=True)
def setup_test_env(monkeypatch):
    """ä¸ºæ¯ä¸ªæµ‹è¯•è®¾ç½®ç¯å¢ƒ"""
    monkeypatch.setenv("TEST_MODE", "true")
    monkeypatch.setenv("LOG_LEVEL", "DEBUG")

# Mock fixtures
@pytest.fixture
def mock_openai_client():
    return AsyncMock()

# é…ç½®fixtures
@pytest.fixture
def test_config():
    return {
        "llm": {
            "provider": "openai",
            "api_key": "test-key",
            "model": "gpt-3.5-turbo"
        },
        "vector_store": {
            "provider": "chroma",
            "collection_name": "test-collection"
        }
    }
```

## ğŸ¯ æµ‹è¯•æœ€ä½³å®è·µ

### 1. æµ‹è¯•å‘½åçº¦å®š
```python
def test_[unit_under_test]_[scenario]_[expected_result]():
    """æµ‹è¯•å‡½æ•°å‘½ååº”è¯¥æ¸…æ™°æè¿°æµ‹è¯•å†…å®¹"""
    pass

# å¥½çš„ä¾‹å­
def test_llm_provider_chat_completion_with_valid_input_returns_response():
def test_vector_store_search_with_empty_query_raises_validation_error():
def test_cache_get_with_expired_key_returns_none():
```

### 2. AAAæ¨¡å¼ï¼ˆArrange-Act-Assertï¼‰
```python
async def test_llm_chat_completion_success():
    # Arrange - å‡†å¤‡æµ‹è¯•æ•°æ®
    provider = OpenAIProvider(test_config)
    messages = [ChatMessage(role="user", content="Hello")]
    
    # Act - æ‰§è¡Œæµ‹è¯•æ“ä½œ
    response = await provider.chat_completion(messages, "gpt-3.5-turbo")
    
    # Assert - éªŒè¯ç»“æœ
    assert response.content is not None
    assert response.model == "gpt-3.5-turbo"
    assert response.usage["total_tokens"] > 0
```

### 3. æµ‹è¯•éš”ç¦»å’Œæ¸…ç†
```python
@pytest.fixture
async def clean_vector_store():
    """ç¡®ä¿æ¯ä¸ªæµ‹è¯•éƒ½æœ‰å¹²å‡€çš„å‘é‡å­˜å‚¨"""
    store = ChromaProvider(test_config)
    await store.initialize()
    
    yield store
    
    # æ¸…ç†æµ‹è¯•æ•°æ®
    await store.clear_collection()
    await store.cleanup()
```

### 4. å‚æ•°åŒ–æµ‹è¯•
```python
@pytest.mark.parametrize("model,expected_provider", [
    ("gpt-3.5-turbo", "openai"),
    ("claude-3", "anthropic"),
    ("llama-2", "huggingface"),
])
def test_model_provider_mapping(model, expected_provider):
    provider = get_provider_for_model(model)
    assert provider.provider_type == expected_provider
```

## ğŸ›¡ï¸ æµ‹è¯•è´¨é‡ä¿è¯

### ä»£ç è¦†ç›–ç‡è¦æ±‚
- **å•å…ƒæµ‹è¯•**: â‰¥ 90%è¦†ç›–ç‡
- **é›†æˆæµ‹è¯•**: â‰¥ 70%è¦†ç›–ç‡  
- **å…³é”®è·¯å¾„**: 100%è¦†ç›–ç‡

### æ€§èƒ½åŸºå‡†
- **LLMè¯·æ±‚**: < 5ç§’å“åº”æ—¶é—´
- **å‘é‡æœç´¢**: < 100msæŸ¥è¯¢æ—¶é—´
- **ç¼“å­˜æ“ä½œ**: < 10msè®¿é—®æ—¶é—´

### æµ‹è¯•æ•°æ®ç®¡ç†
```python
# tests/data/sample_data.py
SAMPLE_DOCUMENTS = [
    VectorDocument(
        id="doc1",
        content="Paris is the capital of France.",
        metadata={"source": "geography", "topic": "capitals"}
    ),
    # æ›´å¤šæ ·æœ¬æ•°æ®...
]

SAMPLE_MESSAGES = [
    ChatMessage(role="user", content="Hello"),
    ChatMessage(role="assistant", content="Hi there!"),
    # æ›´å¤šæ ·æœ¬æ¶ˆæ¯...
]
```

## ğŸ” è°ƒè¯•å’Œæ•…éšœæ’é™¤

### æµ‹è¯•è°ƒè¯•
```python
# ä½¿ç”¨pytestè°ƒè¯•
pytest --pdb                    # æµ‹è¯•å¤±è´¥æ—¶è¿›å…¥è°ƒè¯•å™¨
pytest --pdb-trace             # æµ‹è¯•å¼€å§‹æ—¶è¿›å…¥è°ƒè¯•å™¨
pytest -s                      # æ˜¾ç¤ºprintè¾“å‡º
pytest --lf                    # åªè¿è¡Œä¸Šæ¬¡å¤±è´¥çš„æµ‹è¯•
pytest --tb=short              # ç®€çŸ­çš„é”™è¯¯å›æº¯
```

### æ—¥å¿—é…ç½®
```python
# tests/conftest.py
import logging

@pytest.fixture(autouse=True)
def configure_test_logging():
    """ä¸ºæµ‹è¯•é…ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
```

### CI/CDé›†æˆ
```yaml
# .github/workflows/test.yml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8, 3.9, 3.10, 3.11]
    
    steps:
    - uses: actions/checkout@v2
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v2
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        pip install -e ".[dev]"
    
    - name: Run unit tests
      run: pytest tests/unit/ -v
    
    - name: Run integration tests
      run: pytest tests/integration/ -v
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
    
    - name: Upload coverage
      uses: codecov/codecov-action@v1
```
