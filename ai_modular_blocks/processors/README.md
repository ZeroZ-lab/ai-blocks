# Processors Module

æ–‡æ¡£å¤„ç†å™¨æ¨¡å— - AI Modular Blocksçš„æ•°æ®é¢„å¤„ç†å±‚

## ðŸŽ¯ èŒè´£èŒƒå›´

å®žçŽ°å„ç§æ–‡æ¡£å¤„ç†å’Œæ•°æ®é¢„å¤„ç†åŠŸèƒ½ï¼Œå°†åŽŸå§‹æ•°æ®è½¬æ¢ä¸ºAIç³»ç»Ÿå¯ä»¥å¤„ç†çš„æ ‡å‡†æ ¼å¼ã€‚

## ðŸ“ ç›®å½•ç»„ç»‡

### æ–‡æ¡£åŠ è½½å™¨ (Loaders)
ä»Žå„ç§æ•°æ®æºåŠ è½½åŽŸå§‹æ–‡æ¡£ï¼š

```
loaders/
â”œâ”€â”€ text.py        # çº¯æ–‡æœ¬æ–‡ä»¶åŠ è½½å™¨
â”œâ”€â”€ pdf.py         # PDFæ–‡æ¡£åŠ è½½å™¨
â”œâ”€â”€ html.py        # HTMLç½‘é¡µåŠ è½½å™¨
â”œâ”€â”€ markdown.py    # Markdownæ–‡æ¡£åŠ è½½å™¨
â”œâ”€â”€ office.py      # Officeæ–‡æ¡£åŠ è½½å™¨ï¼ˆWord, Excel, PPTï¼‰
â”œâ”€â”€ code.py        # ä»£ç æ–‡ä»¶åŠ è½½å™¨
â””â”€â”€ database.py    # æ•°æ®åº“è®°å½•åŠ è½½å™¨
```

### æ–‡æ¡£åˆ†å‰²å™¨ (Splitters)
å°†å¤§æ–‡æ¡£æ‹†åˆ†ä¸ºé€‚åˆå¤„ç†çš„å°å—ï¼š

```
splitters/
â”œâ”€â”€ text_splitter.py      # æ–‡æœ¬åˆ†å‰²å™¨
â”œâ”€â”€ recursive_splitter.py # é€’å½’å­—ç¬¦åˆ†å‰²å™¨
â”œâ”€â”€ semantic_splitter.py  # è¯­ä¹‰åˆ†å‰²å™¨
â”œâ”€â”€ code_splitter.py      # ä»£ç åˆ†å‰²å™¨
â””â”€â”€ markdown_splitter.py  # Markdownç»“æž„åˆ†å‰²å™¨
```

### æ–‡æ¡£è½¬æ¢å™¨ (Transformers)
å¯¹æ–‡æ¡£å†…å®¹è¿›è¡Œæ ¼å¼è½¬æ¢å’Œæ¸…ç†ï¼š

```
transformers/
â”œâ”€â”€ cleaner.py         # æ–‡æœ¬æ¸…ç†å™¨
â”œâ”€â”€ normalizer.py      # æ–‡æœ¬æ ‡å‡†åŒ–å™¨
â”œâ”€â”€ extractor.py       # å…³é”®ä¿¡æ¯æå–å™¨
â”œâ”€â”€ enricher.py        # å…ƒæ•°æ®ä¸°å¯Œå™¨
â””â”€â”€ formatter.py       # æ ¼å¼è½¬æ¢å™¨
```

### æ–‡æ¡£è¿‡æ»¤å™¨ (Filters)
åŸºäºŽå„ç§æ¡ä»¶è¿‡æ»¤å’Œç­›é€‰æ–‡æ¡£ï¼š

```
filters/
â”œâ”€â”€ content_filter.py   # å†…å®¹è¿‡æ»¤å™¨
â”œâ”€â”€ metadata_filter.py  # å…ƒæ•°æ®è¿‡æ»¤å™¨
â”œâ”€â”€ quality_filter.py   # è´¨é‡è¿‡æ»¤å™¨
â””â”€â”€ dedup_filter.py     # åŽ»é‡è¿‡æ»¤å™¨
```

## ðŸ”§ æ ¸å¿ƒæŽ¥å£

æ‰€æœ‰å¤„ç†å™¨éƒ½å®žçŽ°`DocumentProcessor`æŽ¥å£ï¼š

```python
from ai_modular_blocks.core.interfaces import DocumentProcessor
from ai_modular_blocks.core.types import DocumentList

class MyProcessor(DocumentProcessor):
    async def process(self, documents: DocumentList) -> DocumentList:
        # å®žçŽ°æ–‡æ¡£å¤„ç†é€»è¾‘
        processed_docs = []
        for doc in documents:
            processed_doc = await self.process_single(doc)
            processed_docs.append(processed_doc)
        return processed_docs
    
    def get_processor_name(self) -> str:
        return "my_processor"
```

## ðŸ“– ä½¿ç”¨ç¤ºä¾‹

### æ–‡æ¡£åŠ è½½å’Œå¤„ç†æµæ°´çº¿
```python
from ai_modular_blocks.processors.loaders import PDFLoader
from ai_modular_blocks.processors.splitters import RecursiveSplitter
from ai_modular_blocks.processors.transformers import TextCleaner
from ai_modular_blocks.processors.filters import QualityFilter

# åˆ›å»ºå¤„ç†æµæ°´çº¿
async def process_pdf_document(file_path: str):
    # 1. åŠ è½½PDFæ–‡æ¡£
    loader = PDFLoader()
    documents = await loader.load(file_path)
    
    # 2. æ¸…ç†æ–‡æœ¬
    cleaner = TextCleaner()
    documents = await cleaner.process(documents)
    
    # 3. åˆ†å‰²æ–‡æ¡£
    splitter = RecursiveSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    documents = await splitter.process(documents)
    
    # 4. è´¨é‡è¿‡æ»¤
    quality_filter = QualityFilter(
        min_length=50,
        max_length=2000
    )
    documents = await quality_filter.process(documents)
    
    return documents
```

### æ‰¹é‡æ–‡æ¡£å¤„ç†
```python
from ai_modular_blocks.processors import ProcessingPipeline

# å®šä¹‰å¤„ç†æµæ°´çº¿
pipeline = ProcessingPipeline([
    PDFLoader(),
    TextCleaner(),
    RecursiveSplitter(chunk_size=1000),
    QualityFilter(min_length=50)
])

# æ‰¹é‡å¤„ç†æ–‡æ¡£
file_paths = ["doc1.pdf", "doc2.pdf", "doc3.pdf"]
all_documents = []

for file_path in file_paths:
    documents = await pipeline.process_file(file_path)
    all_documents.extend(documents)

print(f"å¤„ç†äº† {len(all_documents)} ä¸ªæ–‡æ¡£å—")
```

### è‡ªå®šä¹‰å¤„ç†å™¨
```python
from ai_modular_blocks.core.base import BaseDocumentProcessor
from ai_modular_blocks.core.types import VectorDocument
import re

class EmailProcessor(BaseDocumentProcessor):
    """é‚®ä»¶å†…å®¹å¤„ç†å™¨"""
    
    def __init__(self):
        super().__init__("email_processor")
    
    async def _process_impl(self, documents: DocumentList) -> DocumentList:
        processed = []
        
        for doc in documents:
            # æå–é‚®ä»¶å¤´ä¿¡æ¯
            content = doc.content
            
            # æå–å‘ä»¶äºº
            sender_match = re.search(r'From: (.+)', content)
            sender = sender_match.group(1) if sender_match else "Unknown"
            
            # æå–ä¸»é¢˜
            subject_match = re.search(r'Subject: (.+)', content)
            subject = subject_match.group(1) if subject_match else "No Subject"
            
            # æå–æ­£æ–‡ï¼ˆåŽ»é™¤é‚®ä»¶å¤´ï¼‰
            body_start = content.find('\n\n')
            body = content[body_start+2:] if body_start != -1 else content
            
            # åˆ›å»ºå¤„ç†åŽçš„æ–‡æ¡£
            processed_doc = VectorDocument(
                id=doc.id,
                content=body.strip(),
                metadata={
                    **doc.metadata,
                    "sender": sender,
                    "subject": subject,
                    "content_type": "email"
                },
                content_type=doc.content_type,
                created_at=doc.created_at,
                updated_at=doc.updated_at
            )
            
            processed.append(processed_doc)
        
        return processed
```

## ðŸš€ æ‰©å±•æŒ‡å—

### æ·»åŠ æ–°çš„æ–‡æ¡£åŠ è½½å™¨
1. ç»§æ‰¿`BaseDocumentProcessor`æˆ–å®žçŽ°`DocumentProcessor`æŽ¥å£
2. å®žçŽ°æ–‡æ¡£åŠ è½½é€»è¾‘
3. å¤„ç†å„ç§æ–‡ä»¶æ ¼å¼å’Œç¼–ç 

```python
from ai_modular_blocks.core.base import BaseDocumentProcessor

class JSONLoader(BaseDocumentProcessor):
    def __init__(self):
        super().__init__("json_loader")
    
    async def load(self, file_path: str) -> DocumentList:
        """ä»ŽJSONæ–‡ä»¶åŠ è½½æ–‡æ¡£"""
        import json
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        documents = []
        if isinstance(data, list):
            for i, item in enumerate(data):
                doc = VectorDocument(
                    id=f"{file_path}_{i}",
                    content=json.dumps(item, ensure_ascii=False),
                    metadata={"source": file_path, "index": i}
                )
                documents.append(doc)
        
        return documents
```

### æ·»åŠ æ–°çš„æ–‡æ¡£åˆ†å‰²ç­–ç•¥
```python
class SemanticSplitter(BaseDocumentProcessor):
    """åŸºäºŽè¯­ä¹‰çš„æ–‡æ¡£åˆ†å‰²å™¨"""
    
    def __init__(self, embedding_provider, similarity_threshold=0.8):
        super().__init__("semantic_splitter")
        self.embedding_provider = embedding_provider
        self.similarity_threshold = similarity_threshold
    
    async def _process_impl(self, documents: DocumentList) -> DocumentList:
        split_documents = []
        
        for doc in documents:
            # æŒ‰å¥å­åˆ†å‰²
            sentences = self.split_into_sentences(doc.content)
            
            # ç”Ÿæˆå¥å­åµŒå…¥
            embeddings = await self.embedding_provider.embed_text(sentences)
            
            # åŸºäºŽè¯­ä¹‰ç›¸ä¼¼æ€§åˆå¹¶å¥å­
            chunks = self.merge_by_similarity(sentences, embeddings)
            
            # åˆ›å»ºåˆ†å‰²åŽçš„æ–‡æ¡£
            for i, chunk in enumerate(chunks):
                split_doc = VectorDocument(
                    id=f"{doc.id}_chunk_{i}",
                    content=chunk,
                    metadata={
                        **doc.metadata,
                        "chunk_index": i,
                        "parent_id": doc.id
                    }
                )
                split_documents.append(split_doc)
        
        return split_documents
```

## ðŸŽ¨ è®¾è®¡åŽŸåˆ™

### 1. æµæ°´çº¿æ¨¡å¼
å¤„ç†å™¨å¯ä»¥ç»„åˆæˆæµæ°´çº¿ï¼Œæ•°æ®ä¾æ¬¡é€šè¿‡å„ä¸ªå¤„ç†å™¨ï¼š

```python
# æµæ°´çº¿ç»„åˆ
pipeline = [
    PDFLoader(),
    TextCleaner(),
    RecursiveSplitter(),
    QualityFilter()
]

# ä¾æ¬¡å¤„ç†
documents = input_documents
for processor in pipeline:
    documents = await processor.process(documents)
```

### 2. å¯é…ç½®æ€§
æ¯ä¸ªå¤„ç†å™¨éƒ½æ”¯æŒä¸°å¯Œçš„é…ç½®é€‰é¡¹ï¼š

```python
splitter = RecursiveSplitter(
    chunk_size=1000,           # å—å¤§å°
    chunk_overlap=200,         # é‡å å¤§å°
    separators=['\n\n', '\n'], # åˆ†å‰²ç¬¦ä¼˜å…ˆçº§
    keep_separator=True        # ä¿ç•™åˆ†å‰²ç¬¦
)
```

### 3. å®¹é”™æ€§
å¤„ç†å™¨åº”è¯¥ä¼˜é›…åœ°å¤„ç†å„ç§å¼‚å¸¸æƒ…å†µï¼š

```python
async def _process_impl(self, documents: DocumentList) -> DocumentList:
    processed = []
    errors = []
    
    for doc in documents:
        try:
            processed_doc = await self.process_single(doc)
            processed.append(processed_doc)
        except Exception as e:
            # è®°å½•é”™è¯¯ä½†ç»§ç»­å¤„ç†
            self.logger.error(f"Failed to process document {doc.id}: {e}")
            errors.append({"doc_id": doc.id, "error": str(e)})
    
    # åœ¨metadataä¸­è®°å½•å¤„ç†ç»Ÿè®¡
    if hasattr(self, 'add_processing_stats'):
        self.add_processing_stats({
            "total_docs": len(documents),
            "processed_docs": len(processed),
            "error_count": len(errors),
            "errors": errors
        })
    
    return processed
```

## ðŸ”§ æ€§èƒ½ä¼˜åŒ–

### å¹¶è¡Œå¤„ç†
```python
import asyncio

async def process_documents_parallel(self, documents: DocumentList) -> DocumentList:
    """å¹¶è¡Œå¤„ç†æ–‡æ¡£ä»¥æé«˜æ€§èƒ½"""
    semaphore = asyncio.Semaphore(10)  # é™åˆ¶å¹¶å‘æ•°
    
    async def process_with_semaphore(doc):
        async with semaphore:
            return await self.process_single(doc)
    
    tasks = [process_with_semaphore(doc) for doc in documents]
    processed_docs = await asyncio.gather(*tasks, return_exceptions=True)
    
    # è¿‡æ»¤å¼‚å¸¸ç»“æžœ
    valid_docs = [doc for doc in processed_docs if not isinstance(doc, Exception)]
    return valid_docs
```

### å†…å­˜ä¼˜åŒ–
```python
async def process_large_documents(self, documents: DocumentList) -> DocumentList:
    """æµå¼å¤„ç†å¤§æ–‡æ¡£é›†åˆ"""
    processed = []
    
    # åˆ†æ‰¹å¤„ç†ä»¥æŽ§åˆ¶å†…å­˜ä½¿ç”¨
    batch_size = 100
    for i in range(0, len(documents), batch_size):
        batch = documents[i:i + batch_size]
        batch_result = await self.process_batch(batch)
        processed.extend(batch_result)
        
        # å¯é€‰ï¼šè§¦å‘åžƒåœ¾å›žæ”¶
        if i % 1000 == 0:
            import gc
            gc.collect()
    
    return processed
```

## ðŸŽ¯ æœ€ä½³å®žè·µ

1. **å•ä¸€èŒè´£** - æ¯ä¸ªå¤„ç†å™¨åªåšä¸€ä»¶äº‹
2. **å¹‚ç­‰æ“ä½œ** - é‡å¤å¤„ç†åº”è¯¥äº§ç”Ÿç›¸åŒç»“æžœ
3. **çŠ¶æ€ä¿å­˜** - æ”¯æŒå¤„ç†è¿‡ç¨‹çš„çŠ¶æ€ä¿å­˜å’Œæ¢å¤
4. **è¿›åº¦è¿½è¸ª** - é•¿æ—¶é—´å¤„ç†æä¾›è¿›åº¦ä¿¡æ¯
5. **èµ„æºæ¸…ç†** - åŠæ—¶é‡Šæ”¾æ–‡ä»¶å¥æŸ„å’Œå†…å­˜èµ„æº

## ðŸ” è°ƒè¯•å’Œç›‘æŽ§

### å¤„ç†å™¨é“¾è°ƒè¯•
```python
from ai_modular_blocks.utils.logging import get_logger

logger = get_logger(__name__)

async def debug_pipeline(documents: DocumentList, processors: List[DocumentProcessor]):
    """è°ƒè¯•å¤„ç†å™¨æµæ°´çº¿"""
    current_docs = documents
    
    for i, processor in enumerate(processors):
        logger.info(f"Step {i+1}: {processor.get_processor_name()}")
        logger.info(f"Input: {len(current_docs)} documents")
        
        current_docs = await processor.process(current_docs)
        
        logger.info(f"Output: {len(current_docs)} documents")
        
        if len(current_docs) == 0:
            logger.warning(f"No documents after {processor.get_processor_name()}")
            break
    
    return current_docs
```

### æ€§èƒ½ç›‘æŽ§
```python
from ai_modular_blocks.utils.monitoring import monitor

class MonitoredProcessor(BaseDocumentProcessor):
    
    @monitor.time_operation("document_processing")
    @monitor.count_calls("processor_calls")
    async def process(self, documents: DocumentList) -> DocumentList:
        return await super().process(documents)
```
