# Providers Module

æä¾›å•†å®ç°æ¨¡å— - AI Modular Blocksçš„å…·ä½“å®ç°å±‚

## ğŸ¯ èŒè´£èŒƒå›´

å®ç°core/interfaces/ä¸­å®šä¹‰çš„æ‰€æœ‰æŠ½è±¡æ¥å£ï¼Œæä¾›ä¸å„ç§ç¬¬ä¸‰æ–¹æœåŠ¡å’Œæœ¬åœ°å·¥å…·çš„å…·ä½“é›†æˆã€‚

## ğŸ“ ç›®å½•ç»„ç»‡

### `llm/` - å¤§è¯­è¨€æ¨¡å‹æä¾›å•†
é›†æˆå„å¤§LLMæœåŠ¡å•†çš„å…·ä½“å®ç°ï¼š

```
llm/
â”œâ”€â”€ openai.py      # OpenAI GPTç³»åˆ—æ¨¡å‹
â”œâ”€â”€ anthropic.py   # Anthropic Claudeç³»åˆ—æ¨¡å‹  
â”œâ”€â”€ huggingface.py # HuggingFaceæ¨¡å‹
â”œâ”€â”€ local.py       # æœ¬åœ°éƒ¨ç½²æ¨¡å‹
â””â”€â”€ mock.py        # æµ‹è¯•ç”¨Mockå®ç°
```

**æ”¯æŒåŠŸèƒ½**: èŠå¤©è¡¥å…¨ã€æµå¼å“åº”ã€Function Callingã€æ¨¡å‹åˆ‡æ¢

### `vectorstores/` - å‘é‡å­˜å‚¨æä¾›å•†
é›†æˆå„ç§å‘é‡æ•°æ®åº“çš„å…·ä½“å®ç°ï¼š

```
vectorstores/
â”œâ”€â”€ pinecone.py    # Pineconeäº‘å‘é‡æ•°æ®åº“
â”œâ”€â”€ chroma.py      # Chromaæœ¬åœ°å‘é‡æ•°æ®åº“
â”œâ”€â”€ faiss.py       # Facebook FAISS
â”œâ”€â”€ qdrant.py      # Qdrantå‘é‡æ•°æ®åº“
â””â”€â”€ memory.py      # å†…å­˜å‘é‡å­˜å‚¨ï¼ˆæµ‹è¯•ç”¨ï¼‰
```

**æ”¯æŒåŠŸèƒ½**: å‘é‡å­˜å‚¨ã€ç›¸ä¼¼æ€§æœç´¢ã€å…ƒæ•°æ®è¿‡æ»¤ã€æ‰¹é‡æ“ä½œ

### `embeddings/` - åµŒå…¥æœåŠ¡æä¾›å•†
é›†æˆå„ç§æ–‡æœ¬åµŒå…¥æœåŠ¡çš„å…·ä½“å®ç°ï¼š

```
embeddings/
â”œâ”€â”€ openai.py      # OpenAI Embedding API
â”œâ”€â”€ huggingface.py # HuggingFaceåµŒå…¥æ¨¡å‹
â”œâ”€â”€ sentence_transformers.py # SentenceTransformers
â”œâ”€â”€ cohere.py      # CohereåµŒå…¥æœåŠ¡
â””â”€â”€ local.py       # æœ¬åœ°åµŒå…¥æ¨¡å‹
```

**æ”¯æŒåŠŸèƒ½**: æ–‡æœ¬å‘é‡åŒ–ã€æ‰¹é‡åµŒå…¥ã€ç»´åº¦é…ç½®ã€æ¨¡å‹é€‰æ‹©

### `tools/` - å·¥å…·æä¾›å•†
é›†æˆå„ç§å·¥å…·å’ŒFunction Callingçš„å…·ä½“å®ç°ï¼š

```
tools/
â”œâ”€â”€ builtin/       # å†…ç½®å·¥å…·ï¼ˆè®¡ç®—å™¨ã€æ—¶é—´ç­‰ï¼‰
â”œâ”€â”€ web/           # Webå·¥å…·ï¼ˆæœç´¢ã€çˆ¬è™«ç­‰ï¼‰
â”œâ”€â”€ filesystem/    # æ–‡ä»¶ç³»ç»Ÿå·¥å…·
â”œâ”€â”€ database/      # æ•°æ®åº“å·¥å…·
â””â”€â”€ custom/        # è‡ªå®šä¹‰å·¥å…·æ¡†æ¶
```

**æ”¯æŒåŠŸèƒ½**: å·¥å…·æ³¨å†Œã€å¹¶è¡Œæ‰§è¡Œã€å‚æ•°éªŒè¯ã€ç»“æœå¤„ç†

## ğŸ”§ å®ç°è§„èŒƒ

### 1. ç»§æ‰¿åŸºç¡€ç±»
æ‰€æœ‰Provideréƒ½åº”è¯¥ç»§æ‰¿core/base.pyä¸­çš„åŸºç¡€ç±»ï¼š

```python
from ai_modular_blocks.core.base import BaseLLMProvider
from ai_modular_blocks.core.types import LLMConfig

class OpenAIProvider(BaseLLMProvider):
    def __init__(self, config: LLMConfig):
        super().__init__(config)
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
    
    async def _chat_completion_impl(self, ...):
        # å…·ä½“å®ç°é€»è¾‘
        pass
```

### 2. é…ç½®ç®¡ç†
æ¯ä¸ªProvideréƒ½æœ‰å¯¹åº”çš„é…ç½®ç±»ï¼š

```python
# ä½¿ç”¨é…ç½®
config = LLMConfig(
    api_key="sk-...",
    base_url="https://api.openai.com/v1",
    timeout=30.0,
    max_retries=3
)

provider = OpenAIProvider(config)
```

### 3. é”™è¯¯å¤„ç†
ç»Ÿä¸€çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶ï¼š

```python
from ai_modular_blocks.core.exceptions import (
    AuthenticationException,
    RateLimitException,
    TimeoutException
)

# Providerå†…éƒ¨åº”è¯¥å°†ç¬¬ä¸‰æ–¹å¼‚å¸¸è½¬æ¢ä¸ºæ¡†æ¶å¼‚å¸¸
try:
    response = await openai_client.chat.completions.create(...)
except openai.AuthenticationError as e:
    raise AuthenticationException(str(e), provider_name="OpenAI")
```

## ğŸ“– ä½¿ç”¨ç¤ºä¾‹

### LLM Providerä½¿ç”¨
```python
from ai_modular_blocks.providers.llm import OpenAIProvider
from ai_modular_blocks.core.types import LLMConfig, ChatMessage

# é…ç½®å’Œåˆå§‹åŒ–
config = LLMConfig(api_key="your-api-key")
llm = OpenAIProvider(config)
await llm.initialize()

# ä½¿ç”¨
messages = [ChatMessage(role="user", content="Hello!")]
response = await llm.chat_completion(
    messages=messages,
    model="gpt-3.5-turbo"
)

print(response.content)
```

### Vector Storeä½¿ç”¨
```python
from ai_modular_blocks.providers.vectorstores import PineconeProvider
from ai_modular_blocks.core.types import VectorStoreConfig, VectorDocument

# é…ç½®å’Œåˆå§‹åŒ–
config = VectorStoreConfig(
    api_key="your-pinecone-key",
    index_name="my-index",
    dimension=1536
)
vector_store = PineconeProvider(config)
await vector_store.initialize()

# å­˜å‚¨æ–‡æ¡£
documents = [
    VectorDocument(
        id="doc1",
        content="Some text content",
        metadata={"source": "example"},
        embedding=[0.1, 0.2, ...]  # 1536ç»´å‘é‡
    )
]

result = await vector_store.upsert(documents)
```

### Tool Providerä½¿ç”¨
```python
from ai_modular_blocks.providers.tools import BuiltinToolProvider
from ai_modular_blocks.core.types import ToolCall

# åˆå§‹åŒ–å·¥å…·æä¾›å•†
tools = BuiltinToolProvider()
await tools.initialize()

# æ‰§è¡Œå·¥å…·
tool_call = ToolCall(
    id="call_1",
    name="calculator",
    arguments={"expression": "2 + 2"}
)

result = await tools.execute_tool(tool_call)
print(result.content)  # "4"
```

## ğŸš€ æ‰©å±•æŒ‡å—

### æ·»åŠ æ–°çš„LLM Provider
1. åœ¨`llm/`ç›®å½•åˆ›å»ºæ–°æ–‡ä»¶
2. ç»§æ‰¿`BaseLLMProvider`æˆ–`EnhancedLLMProvider`
3. å®ç°æ‰€æœ‰æŠ½è±¡æ–¹æ³•
4. åœ¨`llm/__init__.py`ä¸­æ³¨å†Œ

```python
# providers/llm/custom_llm.py
from ai_modular_blocks.core.base import BaseLLMProvider

class CustomLLMProvider(BaseLLMProvider):
    async def _chat_completion_impl(self, ...):
        # è‡ªå®šä¹‰å®ç°
        pass
```

### æ·»åŠ æ–°çš„å·¥å…·
1. åœ¨`tools/`å¯¹åº”åˆ†ç±»ç›®å½•ä¸­åˆ›å»ºå·¥å…·
2. å®ç°å·¥å…·å‡½æ•°å’Œå®šä¹‰
3. æ³¨å†Œåˆ°å·¥å…·æä¾›å•†

```python
# providers/tools/builtin/weather.py
from ai_modular_blocks.core.types import ToolDefinition, ToolParameter

def get_weather_tool_definition() -> ToolDefinition:
    return ToolDefinition(
        name="get_weather",
        description="Get current weather",
        parameters=[
            ToolParameter(
                name="location",
                type="string",
                description="City name",
                required=True
            )
        ]
    )

async def get_weather(location: str) -> str:
    # å®ç°å¤©æ°”æŸ¥è¯¢é€»è¾‘
    return f"Weather in {location}: Sunny, 25Â°C"
```

## ğŸ¨ è®¾è®¡åŸåˆ™

### 1. Provideræ— å…³æ€§
ç”¨æˆ·ä»£ç ä¸åº”è¯¥ä¾èµ–ç‰¹å®šProviderçš„å®ç°ç»†èŠ‚ï¼š

```python
# âœ… å¥½çš„è®¾è®¡ - ä¾èµ–æ¥å£
llm: LLMProvider = get_llm_provider(config)
response = await llm.chat_completion(messages, model)

# âŒ åçš„è®¾è®¡ - ä¾èµ–å…·ä½“å®ç°
openai_llm = OpenAIProvider(config)
response = await openai_llm.openai_specific_method()  # ç‰¹å®šæ–¹æ³•ï¼
```

### 2. é…ç½®é©±åŠ¨
æ‰€æœ‰Providerè¡Œä¸ºéƒ½åº”è¯¥é€šè¿‡é…ç½®æ§åˆ¶ï¼š

```python
# é€šè¿‡é…ç½®åˆ‡æ¢Provider
if config.provider_type == "openai":
    llm = OpenAIProvider(config.llm)
elif config.provider_type == "anthropic":
    llm = AnthropicProvider(config.llm)
```

### 3. æ¸è¿›å¼å¢å¼º
æ”¯æŒåŸºç¡€åŠŸèƒ½å’Œå¢å¼ºåŠŸèƒ½çš„åˆ†å±‚å®ç°ï¼š

```python
# åŸºç¡€åŠŸèƒ½
if isinstance(llm, LLMProvider):
    response = await llm.chat_completion(messages, model)

# å¢å¼ºåŠŸèƒ½
if isinstance(llm, EnhancedLLMProvider):
    response = await llm.chat_completion_with_tools(
        messages, tools, model
    )
```

## ğŸ” æµ‹è¯•ç­–ç•¥

æ¯ä¸ªProvideréƒ½åº”è¯¥æœ‰ï¼š
- **å•å…ƒæµ‹è¯•** - æµ‹è¯•å…·ä½“å®ç°é€»è¾‘
- **é›†æˆæµ‹è¯•** - æµ‹è¯•ä¸ç¬¬ä¸‰æ–¹æœåŠ¡çš„é›†æˆ
- **Mockæµ‹è¯•** - æä¾›æµ‹è¯•ç”¨çš„Mockå®ç°
- **æ€§èƒ½æµ‹è¯•** - éªŒè¯å¹¶å‘å’Œååé‡è¡¨ç°

## ğŸ¯ è´¨é‡æ ‡å‡†

- **æ¥å£ä¸€è‡´æ€§** - ä¸¥æ ¼éµå¾ªcore/interfaces/å®šä¹‰
- **é”™è¯¯å¤„ç†** - ç»Ÿä¸€çš„å¼‚å¸¸ç±»å‹å’Œé”™è¯¯ä¿¡æ¯
- **æ—¥å¿—è®°å½•** - è¯¦ç»†çš„æ“ä½œæ—¥å¿—å’Œæ€§èƒ½æŒ‡æ ‡
- **æ–‡æ¡£å®Œæ•´** - æ¯ä¸ªProvideréƒ½æœ‰ä½¿ç”¨ç¤ºä¾‹å’Œé…ç½®è¯´æ˜
- **å‘åå…¼å®¹** - æ–°ç‰ˆæœ¬ä¸ç ´åç°æœ‰API
