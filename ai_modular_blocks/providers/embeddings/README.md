# Embedding Providers

åµŒå…¥æœåŠ¡æä¾›å•†å®ç°

## ğŸ¯ ç›®å½•è¯´æ˜

æœ¬ç›®å½•åŒ…å«å„ç§æ–‡æœ¬åµŒå…¥æœåŠ¡å’Œæ¨¡å‹çš„å…·ä½“å®ç°ï¼Œç»Ÿä¸€å®ç°`EmbeddingProvider`æ¥å£ã€‚

## ğŸ“ æä¾›å•†åˆ—è¡¨

### `openai.py` - OpenAI Embedding API
- **æ¨¡å‹**: text-embedding-ada-002, text-embedding-3-small, text-embedding-3-large
- **ç»´åº¦**: 1536 (ada-002), å¯é…ç½® (3-small/large)
- **ç‰¹ç‚¹**: é«˜è´¨é‡ã€å¤šè¯­è¨€æ”¯æŒã€APIç¨³å®š
- **é€‚ç”¨**: ç”Ÿäº§ç¯å¢ƒã€å¤šè¯­è¨€åº”ç”¨

### `huggingface.py` - HuggingFace Embeddingæ¨¡å‹
- **æ¨¡å‹**: sentence-transformers, BERTç³»åˆ—, è‡ªå®šä¹‰æ¨¡å‹
- **ç»´åº¦**: æ¨¡å‹ç›¸å…³ (384-1024å¸¸è§)
- **ç‰¹ç‚¹**: å¼€æºã€å¯æœ¬åœ°éƒ¨ç½²ã€æ¨¡å‹ä¸°å¯Œ
- **é€‚ç”¨**: ç¦»çº¿ç¯å¢ƒã€è‡ªå®šä¹‰éœ€æ±‚

### `sentence_transformers.py` - SentenceTransformers
- **æ¨¡å‹**: all-MiniLM-L6-v2, all-mpnet-base-v2ç­‰
- **ç»´åº¦**: 384-768
- **ç‰¹ç‚¹**: è½»é‡çº§ã€å¿«é€Ÿæ¨ç†ã€æ˜“éƒ¨ç½²
- **é€‚ç”¨**: å¿«é€ŸåŸå‹ã€èµ„æºå—é™ç¯å¢ƒ

### `cohere.py` - Cohere Embedding API
- **æ¨¡å‹**: embed-english-v3.0, embed-multilingual-v3.0
- **ç»´åº¦**: 1024-4096å¯é…ç½®
- **ç‰¹ç‚¹**: è¯­ä¹‰è´¨é‡é«˜ã€æ”¯æŒå‹ç¼©ã€å¤šè¯­è¨€
- **é€‚ç”¨**: é«˜è´¨é‡éœ€æ±‚ã€å¤šè¯­è¨€åœºæ™¯

### `local.py` - æœ¬åœ°åµŒå…¥æ¨¡å‹
- **æ¨¡å‹**: ONNX, TensorRT, è‡ªå®šä¹‰æ ¼å¼
- **ç»´åº¦**: æ¨¡å‹ç›¸å…³
- **ç‰¹ç‚¹**: å®Œå…¨ç¦»çº¿ã€è‡ªä¸»å¯æ§ã€é«˜æ€§èƒ½
- **é€‚ç”¨**: ç§æœ‰éƒ¨ç½²ã€é«˜å®‰å…¨è¦æ±‚

## ğŸ“– ä½¿ç”¨ç¤ºä¾‹

```python
from ai_modular_blocks.providers.embeddings import OpenAIEmbeddingProvider
from ai_modular_blocks.core.types import EmbeddingConfig

# é…ç½®OpenAIåµŒå…¥æœåŠ¡
config = EmbeddingConfig(
    api_key="sk-...",
    model="text-embedding-3-small",
    dimension=1536
)

# åˆå§‹åŒ–åµŒå…¥æä¾›å•†
embedding = OpenAIEmbeddingProvider(config)
await embedding.initialize()

# åµŒå…¥å•ä¸ªæ–‡æœ¬
texts = ["Hello world", "AI is amazing", "Vector databases are useful"]
embeddings = await embedding.embed_text(texts)

print(f"ç”Ÿæˆäº† {len(embeddings)} ä¸ªåµŒå…¥å‘é‡")
print(f"å‘é‡ç»´åº¦: {len(embeddings[0])}")

# åµŒå…¥æ–‡æ¡£
from ai_modular_blocks.core.types import VectorDocument

documents = [
    VectorDocument(
        id="doc1",
        content="äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ",
        metadata={"language": "zh", "topic": "AI"}
    ),
    VectorDocument(
        id="doc2", 
        content="Machine learning enables intelligent systems",
        metadata={"language": "en", "topic": "ML"}
    )
]

embedded_docs = await embedding.embed_documents(documents)

for doc in embedded_docs:
    print(f"æ–‡æ¡£ {doc.id}: åµŒå…¥ç»´åº¦ {len(doc.embedding)}")
```

## ğŸ”§ å®ç°æŒ‡å—

### åŸºç¡€å®ç°ç»“æ„
```python
from ai_modular_blocks.core.base import BaseEmbeddingProvider
from ai_modular_blocks.core.types import EmbeddingConfig

class MyEmbeddingProvider(BaseEmbeddingProvider):
    def __init__(self, config: EmbeddingConfig):
        super().__init__(config)
        self.client = None
        self.model = config.model
    
    async def _initialize_provider(self):
        """åˆå§‹åŒ–åµŒå…¥æœåŠ¡è¿æ¥"""
        self.client = create_embedding_client(self.config)
    
    async def _embed_text_impl(self, texts: List[str], model: Optional[str], **kwargs):
        """å®ç°æ–‡æœ¬åµŒå…¥é€»è¾‘"""
        model = model or self.model
        
        try:
            # è°ƒç”¨åµŒå…¥APIæˆ–æ¨¡å‹
            response = await self.client.embed(texts, model=model)
            
            # æå–åµŒå…¥å‘é‡
            embeddings = [item.embedding for item in response.data]
            
            return embeddings
            
        except Exception as e:
            raise ProviderException(f"Embedding failed: {e}", provider_name=self.provider_name)
    
    def get_embedding_dimension(self, model: Optional[str] = None) -> int:
        """è¿”å›åµŒå…¥å‘é‡ç»´åº¦"""
        model = model or self.model
        return self._get_model_dimension(model)
```

### æ‰¹é‡å¤„ç†ä¼˜åŒ–
```python
async def embed_text_batched(self, texts: List[str], batch_size: int = 100, **kwargs):
    """æ‰¹é‡å¤„ç†å¤§é‡æ–‡æœ¬"""
    all_embeddings = []
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        
        # å¤„ç†æ‰¹æ¬¡
        batch_embeddings = await self._embed_text_impl(batch, **kwargs)
        all_embeddings.extend(batch_embeddings)
        
        # æ·»åŠ å»¶è¿Ÿé¿å…APIé™æµ
        if i % 1000 == 0:
            await asyncio.sleep(0.1)
    
    return all_embeddings
```

### ç¼“å­˜æ”¯æŒ
```python
async def embed_text_with_cache(self, texts: List[str], **kwargs):
    """å¸¦ç¼“å­˜çš„æ–‡æœ¬åµŒå…¥"""
    cached_embeddings = []
    uncached_texts = []
    uncached_indices = []
    
    # æ£€æŸ¥ç¼“å­˜
    for i, text in enumerate(texts):
        cache_key = self._generate_cache_key(text, kwargs)
        cached_embedding = await self.cache.get(cache_key) if self.cache else None
        
        if cached_embedding:
            cached_embeddings.append((i, cached_embedding))
        else:
            uncached_texts.append(text)
            uncached_indices.append(i)
    
    # å¤„ç†æœªç¼“å­˜çš„æ–‡æœ¬
    if uncached_texts:
        new_embeddings = await self._embed_text_impl(uncached_texts, **kwargs)
        
        # ç¼“å­˜æ–°åµŒå…¥
        if self.cache:
            for text, embedding in zip(uncached_texts, new_embeddings):
                cache_key = self._generate_cache_key(text, kwargs)
                await self.cache.set(cache_key, embedding, ttl=3600)
    
    # åˆå¹¶ç»“æœ
    final_embeddings = [None] * len(texts)
    
    for i, embedding in cached_embeddings:
        final_embeddings[i] = embedding
    
    for i, embedding in zip(uncached_indices, new_embeddings):
        final_embeddings[i] = embedding
    
    return final_embeddings
```

## ğŸš€ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. å¹¶å‘å¤„ç†
```python
import asyncio

async def embed_documents_concurrent(self, documents: DocumentList, max_concurrent: int = 10):
    """å¹¶å‘å¤„ç†æ–‡æ¡£åµŒå…¥"""
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def embed_single_doc(doc):
        async with semaphore:
            texts = [doc.content]
            embeddings = await self._embed_text_impl(texts)
            doc.embedding = embeddings[0]
            return doc
    
    tasks = [embed_single_doc(doc) for doc in documents]
    embedded_docs = await asyncio.gather(*tasks)
    
    return embedded_docs
```

### 2. æ™ºèƒ½åˆ†å—
```python
def chunk_text_for_embedding(self, text: str, max_tokens: int = 8000) -> List[str]:
    """æ™ºèƒ½åˆ†å—é•¿æ–‡æœ¬"""
    # ç®€å•å®ç°ï¼Œå®é™…å¯èƒ½éœ€è¦æ›´å¤æ‚çš„é€»è¾‘
    sentences = text.split('.')
    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        if len(current_chunk) + len(sentence) < max_tokens:
            current_chunk += sentence + "."
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = sentence + "."
    
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks
```

### 3. æ¨¡å‹é¢„çƒ­
```python
async def warmup_model(self):
    """é¢„çƒ­åµŒå…¥æ¨¡å‹"""
    warmup_texts = ["Hello", "Test", "Warmup"]
    await self._embed_text_impl(warmup_texts)
    self.logger.info("Embedding model warmed up")
```

## ğŸ” æ¨¡å‹é€‰æ‹©æŒ‡å—

### OpenAI - é«˜è´¨é‡é€šç”¨æ¨è
- âœ… ä¼˜ç§€çš„è¯­ä¹‰ç†è§£
- âœ… å¤šè¯­è¨€æ”¯æŒè‰¯å¥½
- âœ… APIç¨³å®šå¯é 
- âŒ éœ€è¦ç½‘ç»œè¿æ¥
- âŒ æŒ‰ä½¿ç”¨é‡ä»˜è´¹

### SentenceTransformers - è½»é‡çº§æ¨è
- âœ… å¿«é€Ÿæ¨ç†
- âœ… æ¨¡å‹é€‰æ‹©ä¸°å¯Œ
- âœ… å®Œå…¨å…è´¹
- âŒ è´¨é‡å¯èƒ½ä¸å¦‚å•†ä¸šAPI
- âŒ éœ€è¦æœ¬åœ°éƒ¨ç½²

### Cohere - å¤šè¯­è¨€åœºæ™¯æ¨è
- âœ… å¤šè¯­è¨€è´¨é‡ä¼˜ç§€
- âœ… æ”¯æŒåµŒå…¥å‹ç¼©
- âœ… é’ˆå¯¹æœç´¢ä¼˜åŒ–
- âŒ ç›¸å¯¹è¾ƒæ–°
- âŒ ç”Ÿæ€ç³»ç»Ÿè¾ƒå°

### HuggingFace - è‡ªå®šä¹‰éœ€æ±‚æ¨è
- âœ… æ¨¡å‹ç§ç±»æœ€ä¸°å¯Œ
- âœ… æ”¯æŒå¾®è°ƒ
- âœ… å¼€æºç¤¾åŒºæ´»è·ƒ
- âŒ éœ€è¦æ›´å¤šæŠ€æœ¯çŸ¥è¯†
- âŒ éƒ¨ç½²å¤æ‚åº¦è¾ƒé«˜

## ğŸ§ª è´¨é‡è¯„ä¼°

### è¯­ä¹‰ç›¸ä¼¼æ€§æµ‹è¯•
```python
async def test_semantic_similarity(self):
    """æµ‹è¯•è¯­ä¹‰ç›¸ä¼¼æ€§è´¨é‡"""
    test_pairs = [
        ("ç‹—æ˜¯åŠ¨ç‰©", "çŠ¬ç±»å±äºåŠ¨ç‰©ç•Œ"),      # åº”è¯¥é«˜ç›¸ä¼¼
        ("ä»Šå¤©å¤©æ°”å¾ˆå¥½", "è‚¡ç¥¨å¸‚åœºæ³¢åŠ¨"),    # åº”è¯¥ä½ç›¸ä¼¼
        ("æœºå™¨å­¦ä¹ ", "äººå·¥æ™ºèƒ½"),           # åº”è¯¥ä¸­ç­‰ç›¸ä¼¼
    ]
    
    for text1, text2 in test_pairs:
        emb1 = await self.embed_text([text1])
        emb2 = await self.embed_text([text2])
        
        similarity = cosine_similarity(emb1[0], emb2[0])
        print(f"'{text1}' vs '{text2}': {similarity:.3f}")

def cosine_similarity(vec1, vec2):
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    import numpy as np
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
```

### åµŒå…¥è´¨é‡åŸºå‡†æµ‹è¯•
```python
async def benchmark_embedding_quality(self, test_dataset):
    """åŸºå‡†æµ‹è¯•åµŒå…¥è´¨é‡"""
    results = {
        "avg_processing_time": 0,
        "embedding_dimension": 0,
        "similarity_scores": []
    }
    
    start_time = time.time()
    
    for sample in test_dataset:
        # åµŒå…¥æŸ¥è¯¢å’Œæ–‡æ¡£
        query_emb = await self.embed_text([sample.query])
        doc_embs = await self.embed_text(sample.documents)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = [
            cosine_similarity(query_emb[0], doc_emb) 
            for doc_emb in doc_embs
        ]
        
        results["similarity_scores"].extend(similarities)
    
    results["avg_processing_time"] = (time.time() - start_time) / len(test_dataset)
    results["embedding_dimension"] = len(query_emb[0])
    
    return results
```

## ğŸ¯ æœ€ä½³å®è·µ

1. **æ¨¡å‹é€‰æ‹©** - æ ¹æ®åº”ç”¨åœºæ™¯é€‰æ‹©åˆé€‚çš„æ¨¡å‹
2. **æ‰¹é‡å¤„ç†** - ä½¿ç”¨æ‰¹é‡APIæé«˜æ•ˆç‡
3. **ç¼“å­˜ç­–ç•¥** - ç¼“å­˜å¸¸ç”¨æ–‡æœ¬çš„åµŒå…¥ç»“æœ
4. **é”™è¯¯å¤„ç†** - ä¼˜é›…å¤„ç†APIé™æµå’Œç½‘ç»œé”™è¯¯
5. **ç›‘æ§æŒ‡æ ‡** - è·Ÿè¸ªåµŒå…¥è´¨é‡å’Œæ€§èƒ½æŒ‡æ ‡
6. **æˆæœ¬æ§åˆ¶** - åˆç†ä½¿ç”¨APIä»¥æ§åˆ¶æˆæœ¬
7. **å®‰å…¨è€ƒè™‘** - ä¿æŠ¤APIå¯†é’¥å’Œæ•æ„Ÿæ•°æ®

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜
- **APIé™æµ**: å®ç°é€€é¿é‡è¯•ç­–ç•¥
- **ç»´åº¦ä¸åŒ¹é…**: ç¡®ä¿å‘é‡å­˜å‚¨é…ç½®æ­£ç¡®
- **å†…å­˜æº¢å‡º**: ä½¿ç”¨æ‰¹é‡å¤„ç†å’Œæµå¼å¤„ç†
- **è´¨é‡é—®é¢˜**: é€‰æ‹©æ›´é€‚åˆçš„æ¨¡å‹æˆ–è°ƒæ•´é¢„å¤„ç†
