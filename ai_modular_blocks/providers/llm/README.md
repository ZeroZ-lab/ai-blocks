# LLM Providers

å¤§è¯­è¨€æ¨¡å‹æä¾›å•†å®ç°

## ğŸ¯ ç›®å½•è¯´æ˜

æœ¬ç›®å½•åŒ…å«å„ç§å¤§è¯­è¨€æ¨¡å‹æœåŠ¡å•†çš„å…·ä½“å®ç°ï¼Œç»Ÿä¸€å®ç°`LLMProvider`å’Œ`EnhancedLLMProvider`æ¥å£ã€‚

## ğŸ“ æä¾›å•†åˆ—è¡¨

### `openai.py` - OpenAI GPTç³»åˆ—
- **æ”¯æŒæ¨¡å‹**: GPT-3.5, GPT-4, GPT-4 Turbo
- **æ ¸å¿ƒåŠŸèƒ½**: èŠå¤©è¡¥å…¨ã€Function Callingã€æµå¼å“åº”
- **ç‰¹æ®ŠåŠŸèƒ½**: è§†è§‰æ¨¡å‹æ”¯æŒã€JSONæ¨¡å¼

### `anthropic.py` - Anthropic Claudeç³»åˆ—  
- **æ”¯æŒæ¨¡å‹**: Claude-3 Haiku, Sonnet, Opus
- **æ ¸å¿ƒåŠŸèƒ½**: é•¿ä¸Šä¸‹æ–‡å¤„ç†ã€å®‰å…¨å¯¹è¯
- **ç‰¹æ®ŠåŠŸèƒ½**: ç³»ç»Ÿæç¤ºä¼˜åŒ–ã€æ€ç»´é“¾æ¨ç†

### `deepseek_provider.py` - DeepSeek AIæ¨¡å‹ âœ…
- **æ”¯æŒæ¨¡å‹**: DeepSeek-V3, DeepSeek-Chat, DeepSeek-Coder, DeepSeek-Reasoner
- **æ ¸å¿ƒåŠŸèƒ½**: ä»£ç ç”Ÿæˆã€æ¨ç†èƒ½åŠ›ã€ä¸­è‹±æ–‡å¯¹è¯
- **ç‰¹æ®ŠåŠŸèƒ½**: OpenAI APIå…¼å®¹ã€é«˜æ€§èƒ½æ¨ç†ã€128Kä¸Šä¸‹æ–‡
- **APIæ ¼å¼**: OpenAIå…¼å®¹ï¼Œä½¿ç”¨openaiåŒ…ä½œä¸ºä¾èµ–

### ğŸš§ è®¡åˆ’ä¸­çš„Provider

ä»¥ä¸‹Provideræ­£åœ¨è§„åˆ’ä¸­ï¼Œæ¬¢è¿è´¡çŒ®ï¼š

#### `huggingface_provider.py` - HuggingFaceæ¨¡å‹
- **æ”¯æŒæ¨¡å‹**: å¼€æºLLMï¼ˆLlama, Mistralç­‰ï¼‰
- **æ ¸å¿ƒåŠŸèƒ½**: æœ¬åœ°æ¨ç†ã€æ¨¡å‹å¾®è°ƒ
- **ç‰¹æ®ŠåŠŸèƒ½**: è‡ªå®šä¹‰æ¨¡å‹åŠ è½½ã€GPUåŠ é€Ÿ

#### `local_provider.py` - æœ¬åœ°éƒ¨ç½²æ¨¡å‹  
- **æ”¯æŒæ¡†æ¶**: Ollama, vLLM, TGI
- **æ ¸å¿ƒåŠŸèƒ½**: ç§æœ‰éƒ¨ç½²ã€ç¦»çº¿æ¨ç†
- **ç‰¹æ®ŠåŠŸèƒ½**: èµ„æºæ§åˆ¶ã€æ‰¹é‡æ¨ç†

#### `mock_provider.py` - æµ‹è¯•Mockå®ç°
- **ç”¨é€”**: å•å…ƒæµ‹è¯•ã€å¼€å‘è°ƒè¯•
- **åŠŸèƒ½**: å¯é¢„æµ‹å“åº”ã€å»¶è¿Ÿæ¨¡æ‹Ÿ
- **é…ç½®**: è‡ªå®šä¹‰å“åº”å†…å®¹å’Œå»¶è¿Ÿ

## ğŸ“– ä½¿ç”¨ç¤ºä¾‹

### OpenAI Provider
```python
from ai_modular_blocks.providers.llm import OpenAIProvider
from ai_modular_blocks.core.types import LLMConfig, ChatMessage

# é…ç½®OpenAI
config = LLMConfig(
    api_key="sk-...",
    base_url="https://api.openai.com/v1",  # å¯é€‰
    timeout=30.0,
    max_retries=3
)

# åˆ›å»ºæä¾›å•†ï¼ˆè‡ªåŠ¨åˆå§‹åŒ–ï¼‰
llm = OpenAIProvider(config)

# åŸºç¡€èŠå¤©
messages = [ChatMessage(role="user", content="Hello, AI!")]
response = await llm.chat_completion(messages, model="gpt-3.5-turbo")
```

### DeepSeek Provider
```python
from ai_modular_blocks.providers.llm import DeepSeekProvider
from ai_modular_blocks.core.types import LLMConfig, ChatMessage

# é…ç½®DeepSeek
config = LLMConfig(
    api_key="sk-...",  # DeepSeek API key
    base_url="https://api.deepseek.com/v1",  # é»˜è®¤ç«¯ç‚¹
    timeout=30.0,
    max_retries=3
)

# åˆ›å»ºDeepSeekæä¾›å•†ï¼ˆè‡ªåŠ¨åˆå§‹åŒ–ï¼‰
llm = DeepSeekProvider(config)

# DeepSeekèŠå¤©
messages = [ChatMessage(role="user", content="ç”¨Pythonå†™ä¸€ä¸ªå¿«é€Ÿæ’åº")]
response = await llm.chat_completion(messages, model="deepseek-coder")

# æµå¼å“åº”
async for chunk in llm.stream_chat_completion(messages, model="deepseek-chat"):
    print(chunk.content, end="")
```

### å·¥å‚æ¨¡å¼ä½¿ç”¨
```python
from ai_modular_blocks.providers.llm.factory import LLMProviderFactory

# åˆ—å‡ºå¯ç”¨æä¾›å•†
providers = LLMProviderFactory.get_available_providers()
print(f"å¯ç”¨æä¾›å•†: {providers}")  # ['openai', 'anthropic', 'deepseek']

# åˆ›å»ºDeepSeek provider
config = LLMConfig(api_key="sk-...")
llm = LLMProviderFactory.create_provider("deepseek", config)
```

## âš¡ è‡ªåŠ¨åˆå§‹åŒ–ç‰¹æ€§

**é‡è¦æ”¹è¿›**ï¼šæ‰€æœ‰providerç°åœ¨æ”¯æŒè‡ªåŠ¨åˆå§‹åŒ–ï¼

```python
# âœ… æ–°çš„ä½¿ç”¨æ–¹å¼ - æ— éœ€æ‰‹åŠ¨initialize
llm = DeepSeekProvider(config)
response = await llm.chat_completion(messages, model="deepseek-chat")  # è‡ªåŠ¨åˆå§‹åŒ–

# âŒ æ—§çš„ä½¿ç”¨æ–¹å¼ - ä¸å†éœ€è¦
llm = DeepSeekProvider(config)
await llm.initialize()  # ç°åœ¨å¯ä»¥çœç•¥è¿™ä¸€æ­¥
response = await llm.chat_completion(messages, model="deepseek-chat")
```

**æŠ€æœ¯å®ç°**ï¼š
- æ‰€æœ‰APIè°ƒç”¨æ–¹æ³•ï¼ˆ`chat_completion`ã€`stream_chat_completion`ç­‰ï¼‰ä¼šè‡ªåŠ¨æ£€æŸ¥å¹¶åˆå§‹åŒ–client
- ä½¿ç”¨åŸºç±»çš„`initialize()`æ–¹æ³•ç¡®ä¿ï¼š
  - âœ… é‡å¤åˆå§‹åŒ–ä¿æŠ¤ï¼ˆåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
  - âœ… ç»Ÿä¸€çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
  - âœ… çŠ¶æ€ç®¡ç†ï¼ˆ`_initialized`æ ‡å¿—ï¼‰

## ğŸ”§ å®ç°æŒ‡å—

æ¯ä¸ªProvideréƒ½åº”è¯¥ï¼š

1. **ç»§æ‰¿åŸºç¡€ç±»**
```python
from ai_modular_blocks.core.base import BaseLLMProvider

class MyLLMProvider(BaseLLMProvider):
    def __init__(self, config: LLMConfig):
        super().__init__(config)
```

2. **å®ç°å¿…éœ€æ–¹æ³•**
```python
async def _chat_completion_impl(self, messages, model, temperature, max_tokens, **kwargs):
    # è‡ªåŠ¨åˆå§‹åŒ–æ£€æŸ¥ï¼ˆæ¨èæ¨¡å¼ï¼‰
    if not self.client:
        await self.initialize()
    
    # å…·ä½“APIè°ƒç”¨å®ç°
    # ...
    pass

async def _get_available_models_impl(self):
    # å¦‚æœéœ€è¦APIè°ƒç”¨ï¼Œæ·»åŠ è‡ªåŠ¨åˆå§‹åŒ–
    if not self.client:
        await self.initialize()
    
    # æˆ–è€…ç›´æ¥è¿”å›é™æ€æ¨¡å‹åˆ—è¡¨ï¼ˆå¦‚Anthropicï¼‰
    return self.get_supported_models()
```

3. **é”™è¯¯å¤„ç†è½¬æ¢**
```python
try:
    # è°ƒç”¨ç¬¬ä¸‰æ–¹API
    response = await third_party_api.chat(...)
except ThirdPartyException as e:
    raise ProviderException(f"Chat failed: {e}", provider_name=self.provider_name)
```

## ğŸš€ æ‰©å±•æ–°Provider

å‚è€ƒDeepSeek providerçš„å®ç°æ¨¡å¼ï¼š

1. **åˆ›å»ºProvideræ–‡ä»¶** `providers/llm/my_provider.py`
2. **å®ç°åŸºç¡€æ¥å£** - ç»§æ‰¿`BaseLLMProvider`
3. **æ·»åŠ å·¥å‚æ³¨å†Œ** - åœ¨`factory.py`ä¸­æ·»åŠ å‘ç°é€»è¾‘
4. **æ›´æ–°å¯¼å‡º** - åœ¨`__init__.py`ä¸­æ·»åŠ å¯¼å…¥
5. **ç¼–å†™æµ‹è¯•** - éªŒè¯åŠŸèƒ½æ­£ç¡®æ€§

```python
# my_provider.py - å‚è€ƒDeepSeekå®ç°
class MyLLMProvider(BaseLLMProvider):
    DEFAULT_BASE_URL = "https://api.my-service.com/v1"
    SUPPORTED_MODELS = ["my-model-1", "my-model-2"]
    
    @classmethod
    def is_available(cls) -> bool:
        # æ£€æŸ¥ä¾èµ–å¯ç”¨æ€§
        return True
    
    @classmethod
    def get_supported_models(cls) -> List[str]:
        return cls.SUPPORTED_MODELS.copy()
    
    async def _chat_completion_impl(self, messages, model, **kwargs):
        # å®ç°å…·ä½“çš„èŠå¤©é€»è¾‘
        pass
    
    async def _get_available_models_impl(self):
        return self.SUPPORTED_MODELS.copy()
```
