# LLM Providers

å¤§è¯­è¨€æ¨¡å‹æä¾›å•†å®ç°

## ğŸ¯ ç›®å½•è¯´æ˜

æœ¬ç›®å½•åŒ…å«å„ç§å¤§è¯­è¨€æ¨¡å‹æœåŠ¡å•†çš„å…·ä½“å®ç°ï¼Œç»Ÿä¸€å®ç°`LLMProvider`å’Œ`EnhancedLLMProvider`æ¥å£ã€‚

## ğŸ“ æä¾›å•†åˆ—è¡¨

### `openai.py` - OpenAI GPTç³»åˆ—
- **æ”¯æŒæ¨¡å‹**: GPT-3.5, GPT-4, GPT-4 Turbo
- **æ ¸å¿ƒåŠŸèƒ½**: èŠå¤©è¡¥å…¨ã€Function Callingã€æµå¼å“åº”
- **ç‰¹æ®ŠåŠŸèƒ½**: è§†è§‰æ¨¡å‹æ”¯æŒã€JSONæ¨¡å¼

### `anthropic.py` - Anthropic Claudeç³»åˆ—  
- **æ”¯æŒæ¨¡å‹**: Claude-3 Haiku, Sonnet, Opus
- **æ ¸å¿ƒåŠŸèƒ½**: é•¿ä¸Šä¸‹æ–‡å¤„ç†ã€å®‰å…¨å¯¹è¯
- **ç‰¹æ®ŠåŠŸèƒ½**: ç³»ç»Ÿæç¤ºä¼˜åŒ–ã€æ€ç»´é“¾æ¨ç†

### `huggingface.py` - HuggingFaceæ¨¡å‹
- **æ”¯æŒæ¨¡å‹**: å¼€æºLLMï¼ˆLlama, Mistralç­‰ï¼‰
- **æ ¸å¿ƒåŠŸèƒ½**: æœ¬åœ°æ¨ç†ã€æ¨¡å‹å¾®è°ƒ
- **ç‰¹æ®ŠåŠŸèƒ½**: è‡ªå®šä¹‰æ¨¡å‹åŠ è½½ã€GPUåŠ é€Ÿ

### `local.py` - æœ¬åœ°éƒ¨ç½²æ¨¡å‹
- **æ”¯æŒæ¡†æ¶**: Ollama, vLLM, TGI
- **æ ¸å¿ƒåŠŸèƒ½**: ç§æœ‰éƒ¨ç½²ã€ç¦»çº¿æ¨ç†
- **ç‰¹æ®ŠåŠŸèƒ½**: èµ„æºæ§åˆ¶ã€æ‰¹é‡æ¨ç†

### `mock.py` - æµ‹è¯•Mockå®ç°
- **ç”¨é€”**: å•å…ƒæµ‹è¯•ã€å¼€å‘è°ƒè¯•
- **åŠŸèƒ½**: å¯é¢„æµ‹å“åº”ã€å»¶è¿Ÿæ¨¡æ‹Ÿ
- **é…ç½®**: è‡ªå®šä¹‰å“åº”å†…å®¹å’Œå»¶è¿Ÿ

## ğŸ“– ä½¿ç”¨ç¤ºä¾‹

```python
from ai_modular_blocks.providers.llm import OpenAIProvider
from ai_modular_blocks.core.types import LLMConfig, ChatMessage

# é…ç½®OpenAI
config = LLMConfig(
    api_key="sk-...",
    base_url="https://api.openai.com/v1",  # å¯é€‰
    timeout=30.0,
    max_retries=3
)

# åˆå§‹åŒ–æä¾›å•†
llm = OpenAIProvider(config)
await llm.initialize()

# åŸºç¡€èŠå¤©
messages = [ChatMessage(role="user", content="Hello, AI!")]
response = await llm.chat_completion(messages, model="gpt-3.5-turbo")

# Function Callingï¼ˆå¦‚æœæ”¯æŒï¼‰
if isinstance(llm, EnhancedLLMProvider):
    tools = [weather_tool_definition]
    response = await llm.chat_completion_with_tools(
        messages=messages,
        tools=tools,
        model="gpt-3.5-turbo"
    )
```

## ğŸ”§ å®ç°æŒ‡å—

æ¯ä¸ªProvideréƒ½åº”è¯¥ï¼š

1. **ç»§æ‰¿åŸºç¡€ç±»**
```python
from ai_modular_blocks.core.base import BaseLLMProvider

class MyLLMProvider(BaseLLMProvider):
    def __init__(self, config: LLMConfig):
        super().__init__(config)
```

2. **å®ç°å¿…éœ€æ–¹æ³•**
```python
async def _chat_completion_impl(self, messages, model, temperature, max_tokens, **kwargs):
    # å…·ä½“å®ç°
    pass

async def _get_available_models_impl(self):
    # è¿”å›æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
    pass
```

3. **é”™è¯¯å¤„ç†è½¬æ¢**
```python
try:
    # è°ƒç”¨ç¬¬ä¸‰æ–¹API
    response = await third_party_api.chat(...)
except ThirdPartyException as e:
    raise ProviderException(f"Chat failed: {e}", provider_name=self.provider_name)
```

## ğŸš€ æ‰©å±•æ–°Provider

1. åˆ›å»ºæ–°æ–‡ä»¶ `providers/llm/my_provider.py`
2. å®ç°`BaseLLMProvider`æˆ–`EnhancedLLMProvider` 
3. åœ¨`__init__.py`ä¸­æ³¨å†Œ
4. æ·»åŠ é…ç½®ç±»å’Œæµ‹è¯•

```python
# my_provider.py
class MyLLMProvider(BaseLLMProvider):
    async def _chat_completion_impl(self, messages, model, **kwargs):
        # å®ç°èŠå¤©é€»è¾‘
        pass
    
    async def _get_available_models_impl(self):
        return ["my-model-1", "my-model-2"]
```
