# Caching Module

ç¼“å­˜ç³»ç»Ÿå®ç°

## ğŸ¯ æ¨¡å—è¯´æ˜

æä¾›å¤šç§ç¼“å­˜åç«¯å’Œç­–ç•¥ï¼Œå®ç°`CacheProvider`æ¥å£ï¼Œæ”¯æŒLLMå“åº”ç¼“å­˜ã€å‘é‡æœç´¢ç¼“å­˜ç­‰åœºæ™¯ã€‚

## ğŸ“ æ–‡ä»¶ç»„ç»‡

### `memory.py` - å†…å­˜ç¼“å­˜
- **ç‰¹ç‚¹**: å¿«é€Ÿè®¿é—®ã€è¿›ç¨‹å†…å…±äº«
- **é€‚ç”¨**: å¼€å‘æµ‹è¯•ã€å•æœºéƒ¨ç½²
- **å®ç°**: åŸºäºPythonå­—å…¸+LRUç­–ç•¥

### `redis.py` - Redisç¼“å­˜  
- **ç‰¹ç‚¹**: åˆ†å¸ƒå¼ã€æŒä¹…åŒ–ã€é«˜æ€§èƒ½
- **é€‚ç”¨**: ç”Ÿäº§ç¯å¢ƒã€å¤šå®ä¾‹éƒ¨ç½²
- **å®ç°**: Rediså®¢æˆ·ç«¯å°è£…

### `file.py` - æ–‡ä»¶ç³»ç»Ÿç¼“å­˜
- **ç‰¹ç‚¹**: æŒä¹…åŒ–ã€æ— ä¾èµ–ã€ç®€å•
- **é€‚ç”¨**: ç¦»çº¿ç¯å¢ƒã€ä¸´æ—¶ç¼“å­˜
- **å®ç°**: æ–‡ä»¶ç³»ç»Ÿç›®å½•ç»“æ„

### `manager.py` - ç¼“å­˜ç®¡ç†å™¨
- **åŠŸèƒ½**: ç»Ÿä¸€ç¼“å­˜æ¥å£ã€è‡ªåŠ¨é€‰æ‹©åç«¯
- **ç‰¹ç‚¹**: é…ç½®é©±åŠ¨ã€çƒ­åˆ‡æ¢æ”¯æŒ

### `strategies.py` - ç¼“å­˜ç­–ç•¥
- **LRU**: æœ€è¿‘æœ€å°‘ä½¿ç”¨
- **TTL**: æ—¶é—´è¿‡æœŸç­–ç•¥  
- **LFU**: æœ€å°‘ä½¿ç”¨é¢‘ç‡

## ğŸ“– ä½¿ç”¨ç¤ºä¾‹

```python
from ai_modular_blocks.utils.caching import CacheManager

# åˆ›å»ºç¼“å­˜ç®¡ç†å™¨
cache_config = {
    "type": "redis",
    "url": "redis://localhost:6379",
    "default_ttl": 3600
}

cache = CacheManager.create_cache(cache_config)

# åŸºç¡€æ“ä½œ
await cache.set("key", "value", ttl=300)
value = await cache.get("key")
await cache.delete("key")

# LLMå“åº”ç¼“å­˜
async def cached_llm_call(messages, model):
    cache_key = f"llm:{model}:{hash(str(messages))}"
    
    # å°è¯•ç¼“å­˜
    cached = await cache.get(cache_key)
    if cached:
        return LLMResponse.from_dict(cached)
    
    # è°ƒç”¨LLM
    response = await llm.chat_completion(messages, model)
    
    # ç¼“å­˜ç»“æœ
    await cache.set(cache_key, response.to_dict(), ttl=1800)
    return response
```
