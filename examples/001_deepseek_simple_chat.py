#!/usr/bin/env python3
"""
æœ€ç®€å•çš„DeepSeekæµå¼èŠå¤©ç¤ºä¾‹

å±•ç¤ºé‡æ„åçš„AI Modular Blocksæ¶æ„å¦‚ä½•ä¼˜é›…åœ°å·¥ä½œï¼š
- ä½¿ç”¨é‡æ„åçš„base.llmæ¨¡å—  
- å±•ç¤ºDeepSeek providerçš„æµå¼å“åº”
- ä½“ç°"Do One Thing and Do It Well"çš„è®¾è®¡

è¿è¡Œå‰è¯·ç¡®ä¿ï¼š
1. pip install openai  # DeepSeekä½¿ç”¨OpenAIå…¼å®¹æ ¼å¼
2. è®¾ç½®ç¯å¢ƒå˜é‡ DEEPSEEK_API_KEY
"""

import asyncio
import os
import sys
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

# ä½¿ç”¨é‡æ„åçš„æ¸…æ™°å¯¼å…¥ï¼ˆç›´æ¥ä»æ¨¡å—å¯¼å…¥é¿å…yamlä¾èµ–ï¼‰
from ai_modular_blocks.core.types.config import LLMConfig
from ai_modular_blocks.core.types.basic import ChatMessage
from ai_modular_blocks.providers.llm.deepseek_provider import DeepSeekProvider

load_dotenv()

async def main():
    """
    æœ€ç®€å•çš„DeepSeekæµå¼èŠå¤©ç¤ºä¾‹
    """
    print("ğŸš€ DeepSeek æµå¼èŠå¤©ç¤ºä¾‹")
    print("=" * 50)
    
    # 1. é…ç½®DeepSeekï¼ˆæœ€ç®€é…ç½®ï¼‰
    api_key = os.getenv("DEEPSEEK_API_KEY")
    
    config = LLMConfig(
        api_key=api_key,
        timeout=30.0,
        max_retries=3
    )
    
    # 2. åˆ›å»ºDeepSeek providerï¼ˆå±•ç¤ºé‡æ„åçš„æ¶æ„ï¼‰
    try:
        llm = DeepSeekProvider(config)
        print("âœ… DeepSeek provideråˆ›å»ºæˆåŠŸ")
        
        # å±•ç¤ºproviderä¿¡æ¯
        info = llm.get_provider_info()
        print(f"ğŸ“‹ Providerä¿¡æ¯:")
        print(f"   - æ¨¡å‹æ”¯æŒ: {info['supported_models']}")
        print(f"   - æµå¼æ”¯æŒ: {info['supports_streaming']}")
        print(f"   - APIæ ¼å¼: {info['api_format']}")
        
    except Exception as e:
        print(f"âŒ DeepSeek provideråˆ›å»ºå¤±è´¥: {e}")
        if "openai" in str(e).lower():
            print("\nğŸ’¡ å®‰è£…ä¾èµ–: pip install openai")
        return
    
    # 3. åˆ›å»ºèŠå¤©æ¶ˆæ¯ï¼ˆè®©AIä»‹ç»è‡ªå·±ï¼‰
    messages = [
        ChatMessage(
            role="user", 
            content="è¯·ç”¨ä¸­æ–‡ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ï¼ŒåŒ…æ‹¬ä½ çš„èƒ½åŠ›å’Œç‰¹ç‚¹ã€‚"
        )
    ]
    
    print(f"\nğŸ¤– å‘DeepSeekå‘é€æ¶ˆæ¯: {messages[0].content}")
    print("\nğŸ“¡ DeepSeekæµå¼å›å¤:")
    print("-" * 50)
    
    # 4. æµå¼èŠå¤©ï¼ˆå±•ç¤ºçœŸæ­£çš„èƒ½åŠ›ï¼‰
    try:
        full_response = ""
        
        async for chunk in llm.stream_chat_completion(
            messages=messages,
            model="deepseek-chat",  # ä½¿ç”¨deepseek-chatæ¨¡å‹
            temperature=0.7
        ):
            # æ‰“å°æµå¼å†…å®¹
            content = chunk.content
            if content:
                print(content, end="", flush=True)
                full_response += content
        
        print(f"\n{'-' * 50}")
        print(f"âœ… æµå¼å“åº”å®Œæˆï¼Œæ€»é•¿åº¦: {len(full_response)} å­—ç¬¦")
        
    except Exception as e:
        print(f"âŒ æµå¼èŠå¤©å¤±è´¥: {e}")
        
        # æä¾›å…·ä½“çš„é”™è¯¯å¤„ç†å»ºè®®
        if "authentication" in str(e).lower():
            print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ: è¯·æ£€æŸ¥API keyæ˜¯å¦æ­£ç¡®")
        elif "openai" in str(e).lower():
            print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ: pip install openai")
        elif "network" in str(e).lower():
            print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ: æ£€æŸ¥ç½‘ç»œè¿æ¥")
        else:
            print(f"\nğŸ’¡ é”™è¯¯è¯¦æƒ…: {type(e).__name__}")


if __name__ == "__main__":
    print("ğŸŒŸ AI Modular Blocks - DeepSeekæµå¼èŠå¤©ç¤ºä¾‹")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰openaiä¾èµ–
    try:
        import openai
        has_openai = True
    except ImportError:
        has_openai = False
    
    if not has_openai:
        print("âš ï¸  ç¼ºå°‘openaiä¾èµ–åŒ…")
        print("ğŸ’¡ å®‰è£…å‘½ä»¤: pip install openai")
        print()
    else:
        # è¿è¡ŒçœŸå®ç¤ºä¾‹
        try:
            asyncio.run(main())
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼")
        except Exception as e:
            print(f"\nâŒ ç¨‹åºå¼‚å¸¸: {e}")
    
    print("\nğŸ‰ ç¤ºä¾‹å®Œæˆï¼")
    print("ğŸ“š æ›´å¤šç¤ºä¾‹è¯·æŸ¥çœ‹ examples/ ç›®å½•")
