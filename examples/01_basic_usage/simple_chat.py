#!/usr/bin/env python3
"""
æœ€åŸºç¡€çš„LLMèŠå¤©ç¤ºä¾‹

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºå¦‚ä½•ä½¿ç”¨ai-modular-blocksè¿›è¡Œæœ€ç®€å•çš„LLMå¯¹è¯ã€‚
ä¸éœ€è¦å¤æ‚é…ç½®ï¼Œç›´æ¥å¯ç”¨ã€‚
"""

import asyncio
import os
from ai_modular_blocks.providers.llm import LLMProviderFactory
from ai_modular_blocks.core.types import LLMConfig, ChatMessage


async def simple_chat_example():
    """æœ€ç®€å•çš„èŠå¤©ç¤ºä¾‹"""
    
    # 1. åˆå§‹åŒ–å·¥å‚
    LLMProviderFactory.initialize()
    
    # 2. åˆ›å»ºOpenAI providerï¼ˆéœ€è¦è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡ï¼‰
    if not os.getenv("OPENAI_API_KEY"):
        print("é”™è¯¯: è¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
        return
    
    config = LLMConfig(
        provider="openai",
        model="gpt-3.5-turbo",
        api_key=os.getenv("OPENAI_API_KEY"),
        max_tokens=100,
        temperature=0.7
    )
    
    llm = LLMProviderFactory.create_provider("openai", config)
    
    # 3. å‘é€ç®€å•æ¶ˆæ¯
    messages = [
        ChatMessage(role="user", content="ä½ å¥½ï¼è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚")
    ]
    
    print("ğŸ¤– æ­£åœ¨æ€è€ƒ...")
    response = await llm.generate(messages)
    
    print(f"âœ… å›å¤: {response.content}")
    print(f"ğŸ“Š Tokenä½¿ç”¨: {response.usage}")


if __name__ == "__main__":
    # è¿è¡Œç¤ºä¾‹
    print("=== AI Modular Blocks - åŸºç¡€èŠå¤©ç¤ºä¾‹ ===")
    asyncio.run(simple_chat_example())
