"""
002 - å¤šæä¾›å•†æ¨¡å‹å¯¹æ¯”ç¤ºä¾‹

å±•ç¤ºå¦‚ä½•ç”¨ç›¸åŒçš„ä»£ç è°ƒç”¨ä¸åŒçš„LLMæä¾›å•†ï¼š
- ç»Ÿä¸€çš„æ¥å£ï¼Œæ— éœ€å­¦ä¹ ä¸åŒçš„API
- ç”¨æˆ·å¯ä»¥è½»æ¾åˆ‡æ¢å’Œå¯¹æ¯”ä¸åŒæ¨¡å‹
- çº¯Pythonå®ç°ï¼Œæ— æ¡†æ¶ç‰¹æ®Šè¯­æ³•
"""

import asyncio
import os
from dotenv import load_dotenv

from ai_modular_blocks import create_llm

load_dotenv()


class ModelComparator:
    """ç”¨æˆ·è‡ªå®šä¹‰çš„æ¨¡å‹å¯¹æ¯”å™¨ - æ— éœ€ç»§æ‰¿ä»»ä½•æ¡†æ¶ç±»ï¼"""
    
    def __init__(self):
        # é…ç½®ä¸åŒçš„æä¾›å•†
        self.models = {}
        
        # OpenAI
        if os.getenv("OPENAI_API_KEY"):
            self.models["OpenAI GPT-3.5"] = create_llm(
                "openai",
                api_key=os.getenv("OPENAI_API_KEY"),
                model="gpt-4o-mini",
                temperature=0.7,
                base_url="https://api.ephone.ai/v1"
            )
        
        # Anthropic
        if os.getenv("ANTHROPIC_API_KEY"):
            self.models["Anthropic Claude"] = create_llm(
                "openai", 
                api_key=os.getenv("OPENAI_API_KEY"),
                model="claude-3-5-haiku-latest",
                temperature=0.7,
                base_url="https://api.ephone.ai/v1"
            )
        
        # DeepSeek
        if os.getenv("DEEPSEEK_API_KEY"):
            self.models["DeepSeek Chat"] = create_llm(
                "deepseek",
                api_key=os.getenv("DEEPSEEK_API_KEY"), 
                model="deepseek-chat",
                temperature=0.7,
                base_url="https://api.deepseek.com"
            )
    
    async def compare_responses(self, question: str):
        """å¯¹æ¯”ä¸åŒæ¨¡å‹å¯¹åŒä¸€é—®é¢˜çš„å›ç­”"""
        print(f"ğŸ¤” é—®é¢˜: {question}")
        print("=" * 60)
        
        results = {}
        
        for model_name, llm in self.models.items():
            print(f"\nğŸ¤– {model_name}:")
            print("-" * 40)
            
            try:
                response = await llm.generate(question)
                answer = response["content"]
                
                # è®°å½•ç»“æœ
                results[model_name] = {
                    "answer": answer,
                    "length": len(answer),
                    "usage": response.get("usage", {})
                }
                
                # æ˜¾ç¤ºå›ç­”ï¼ˆæˆªå–å‰200å­—ç¬¦ï¼‰
                display_answer = answer[:200] + "..." if len(answer) > 200 else answer
                print(display_answer)
                
                # æ˜¾ç¤ºä½¿ç”¨ç»Ÿè®¡
                if response.get("usage"):
                    print(f"\nğŸ“Š ä½¿ç”¨ç»Ÿè®¡: {response['usage']}")
                
            except Exception as e:
                print(f"âŒ è°ƒç”¨å¤±è´¥: {e}")
                results[model_name] = {"error": str(e)}
        
        return results
    
    def analyze_results(self, results: dict):
        """åˆ†æå¯¹æ¯”ç»“æœ"""
        print(f"\n{'=' * 60}")
        print("ğŸ“ˆ å¯¹æ¯”åˆ†æ:")
        
        successful_results = {k: v for k, v in results.items() if "error" not in v}
        
        if not successful_results:
            print("âŒ æ‰€æœ‰æ¨¡å‹éƒ½è°ƒç”¨å¤±è´¥")
            return
        
        # æ¯”è¾ƒå›ç­”é•¿åº¦
        lengths = [(k, v["length"]) for k, v in successful_results.items()]
        lengths.sort(key=lambda x: x[1], reverse=True)
        
        print(f"\nğŸ“ å›ç­”é•¿åº¦å¯¹æ¯”:")
        for model_name, length in lengths:
            print(f"  {model_name}: {length} å­—ç¬¦")
        
        # æ¯”è¾ƒtokenä½¿ç”¨ï¼ˆå¦‚æœæœ‰ï¼‰
        print(f"\nğŸ”¢ Tokenä½¿ç”¨å¯¹æ¯”:")
        for model_name, result in successful_results.items():
            usage = result.get("usage")
            if usage:
                total_tokens = usage.get("total_tokens", "æœªçŸ¥")
                print(f"  {model_name}: {total_tokens} tokens")
            else:
                print(f"  {model_name}: æ— ä½¿ç”¨ç»Ÿè®¡")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¤šæä¾›å•†æ¨¡å‹å¯¹æ¯”ç¤ºä¾‹")
    print("å±•ç¤ºç»Ÿä¸€æ¥å£è°ƒç”¨ä¸åŒLLMæä¾›å•†")
    print()
    
    # åˆ›å»ºå¯¹æ¯”å™¨ - ç”¨æˆ·è‡ªå®šä¹‰çš„ç±»ï¼Œæ— éœ€ç»§æ‰¿
    comparator = ModelComparator()
    
    if not comparator.models:
        print("âš ï¸  æœªæ£€æµ‹åˆ°ä»»ä½•æœ‰æ•ˆçš„APIå¯†é’¥")
        print("è¯·è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ä¸­çš„è‡³å°‘ä¸€ä¸ª:")
        print("- OPENAI_API_KEY")
        print("- ANTHROPIC_API_KEY") 
        print("- DEEPSEEK_API_KEY")
        return
    
    print(f"âœ… å·²é…ç½® {len(comparator.models)} ä¸ªæ¨¡å‹:")
    for model_name in comparator.models:
        print(f"  - {model_name}")
    
    # æµ‹è¯•é—®é¢˜
    questions = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿè¯·ç”¨ä¸€å¥è¯æ¦‚æ‹¬",
        "å†™ä¸€ä¸ªPythonå‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—",
        "æ¨è5ä¸ªå­¦ä¹ ç¼–ç¨‹çš„ç½‘ç«™"
    ]
    
    # å¯¹æ¯”ä¸åŒé—®é¢˜
    for i, question in enumerate(questions, 1):
        print(f"\nğŸ” æµ‹è¯• {i}/{len(questions)}")
        results = await comparator.compare_responses(question)
        comparator.analyze_results(results)
        
        if i < len(questions):
            print(f"\n{'â¸ï¸ ' * 20}")
            await asyncio.sleep(1)  # é¿å…APIé™æµ


if __name__ == "__main__":
    print("ğŸŒŸ AI Modular Blocks - å¤šæä¾›å•†å¯¹æ¯”")
    print("åŒä¸€å¥—ä»£ç ï¼Œè°ƒç”¨ä¸åŒçš„LLMæä¾›å•†")
    print()
    
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå¼‚å¸¸: {e}")
    
    print("\nğŸ¯ æ ¸å¿ƒä¼˜åŠ¿:")
    print("- ç»Ÿä¸€çš„APIæ¥å£ï¼Œæ— éœ€å­¦ä¹ ä¸åŒæä¾›å•†çš„SDK")
    print("- è½»æ¾åˆ‡æ¢å’Œå¯¹æ¯”ä¸åŒæ¨¡å‹")
    print("- ç”¨æˆ·å¯ä»¥è‡ªç”±å®šä¹‰å¯¹æ¯”é€»è¾‘å’Œåˆ†ææ–¹å¼")