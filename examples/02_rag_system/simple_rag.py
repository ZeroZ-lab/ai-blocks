#!/usr/bin/env python3
"""
ç®€å•RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) ç¤ºä¾‹

å±•ç¤ºå¦‚ä½•æ„å»ºä¸€ä¸ªåŸºç¡€çš„RAGç³»ç»Ÿï¼š
1. æ–‡æ¡£å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
2. ç”¨æˆ·æé—®æ—¶æ£€ç´¢ç›¸å…³æ–‡æ¡£  
3. å°†æ£€ç´¢ç»“æœä½œä¸ºä¸Šä¸‹æ–‡ä¼ ç»™LLMç”Ÿæˆå›ç­”

è¿™æ˜¯æœ€å®ç”¨çš„AIåº”ç”¨æ¨¡å¼ä¹‹ä¸€ã€‚
"""

import asyncio
import os
from ai_modular_blocks.providers.llm import LLMProviderFactory
from ai_modular_blocks.providers.vectorstores import VectorStoreFactory
from ai_modular_blocks.core.types import (
    LLMConfig, VectorStoreConfig, ChatMessage, VectorDocument
)


# ç¤ºä¾‹æ–‡æ¡£æ•°æ® - å…³äºPythonç¼–ç¨‹çš„çŸ¥è¯†
SAMPLE_DOCS = [
    {
        "content": "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œå…·æœ‰ç®€æ´æ˜“è¯»çš„è¯­æ³•ã€‚å®ƒç”±Guido van Rossumäº1991å¹´åˆ›å»ºã€‚",
        "metadata": {"topic": "python_basics", "source": "intro"}
    },
    {
        "content": "Pythonæ”¯æŒå¤šç§ç¼–ç¨‹èŒƒå¼ï¼ŒåŒ…æ‹¬é¢å‘å¯¹è±¡ã€å‡½æ•°å¼å’Œè¿‡ç¨‹å¼ç¼–ç¨‹ã€‚",
        "metadata": {"topic": "python_paradigms", "source": "concepts"}
    },
    {
        "content": "pipæ˜¯Pythonçš„åŒ…ç®¡ç†å™¨ï¼Œç”¨äºå®‰è£…å’Œç®¡ç†PythonåŒ…ã€‚ä½¿ç”¨pip install package_nameæ¥å®‰è£…åŒ…ã€‚",
        "metadata": {"topic": "python_tools", "source": "tools"}
    },
    {
        "content": "è™šæ‹Ÿç¯å¢ƒæ˜¯Pythonå¼€å‘çš„æœ€ä½³å®è·µï¼Œå¯ä»¥ä½¿ç”¨venvæˆ–condaæ¥åˆ›å»ºéš”ç¦»çš„Pythonç¯å¢ƒã€‚",
        "metadata": {"topic": "python_environment", "source": "best_practices"}
    }
]


async def setup_rag_system():
    """åˆå§‹åŒ–RAGç³»ç»Ÿ"""
    
    # 1. åˆå§‹åŒ–å·¥å‚
    LLMProviderFactory.initialize()
    VectorStoreFactory.initialize()
    
    # 2. æ£€æŸ¥ä¾èµ–
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ è¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
        return None, None
    
    # 3. åˆ›å»ºLLM provider
    llm_config = LLMConfig(
        provider="openai",
        model="gpt-3.5-turbo",
        api_key=os.getenv("OPENAI_API_KEY"),
        max_tokens=200,
        temperature=0.3  # è¾ƒä½æ¸©åº¦ä¿è¯å›ç­”å‡†ç¡®æ€§
    )
    llm = LLMProviderFactory.create_provider("openai", llm_config)
    
    # 4. åˆ›å»ºå‘é‡å­˜å‚¨ï¼ˆä½¿ç”¨ChromaDBï¼Œæœ¬åœ°è¿è¡Œï¼‰
    vector_config = VectorStoreConfig(
        provider="chroma",
        collection_name="python_knowledge",
        persist_directory="./chroma_db"  # æœ¬åœ°å­˜å‚¨
    )
    vector_store = VectorStoreFactory.create_provider("chroma", vector_config)
    
    return llm, vector_store


async def load_documents(vector_store):
    """åŠ è½½ç¤ºä¾‹æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“"""
    
    print("ğŸ“š æ­£åœ¨åŠ è½½æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“...")
    
    # è½¬æ¢ä¸ºVectorDocumentæ ¼å¼
    docs = []
    for i, doc in enumerate(SAMPLE_DOCS):
        vector_doc = VectorDocument(
            id=f"doc_{i}",
            content=doc["content"],
            metadata=doc["metadata"],
            # æ³¨æ„ï¼šè¿™é‡Œembeddingä¼šç”±vector storeè‡ªåŠ¨ç”Ÿæˆ
        )
        docs.append(vector_doc)
    
    # æ‰¹é‡æ·»åŠ æ–‡æ¡£
    await vector_store.add_documents(docs)
    print(f"âœ… å·²åŠ è½½ {len(docs)} ä¸ªæ–‡æ¡£")


async def rag_query(question: str, llm, vector_store):
    """æ‰§è¡ŒRAGæŸ¥è¯¢"""
    
    print(f"\nâ“ ç”¨æˆ·é—®é¢˜: {question}")
    
    # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
    print("ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£...")
    search_results = await vector_store.search(
        query=question,
        limit=2,  # åªå–æœ€ç›¸å…³çš„2ä¸ªæ–‡æ¡£
        threshold=0.5  # ç›¸ä¼¼åº¦é˜ˆå€¼
    )
    
    if not search_results.documents:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
        return
    
    # 2. æ„å»ºä¸Šä¸‹æ–‡
    context = "\n".join([
        f"æ–‡æ¡£{i+1}: {doc.content}" 
        for i, doc in enumerate(search_results.documents)
    ])
    
    print(f"ğŸ“„ æ‰¾åˆ° {len(search_results.documents)} ä¸ªç›¸å…³æ–‡æ¡£")
    
    # 3. æ„é€ æç¤ºè¯
    prompt = f"""åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œå¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ— æ³•å›ç­”ã€‚

æ–‡æ¡£å†…å®¹:
{context}

ç”¨æˆ·é—®é¢˜: {question}

è¯·ç®€æ´å‡†ç¡®åœ°å›ç­”:"""

    # 4. ç”Ÿæˆå›ç­”
    messages = [ChatMessage(role="user", content=prompt)]
    response = await llm.generate(messages)
    
    print(f"ğŸ¤– RAGå›ç­”: {response.content}")
    return response.content


async def interactive_rag_demo():
    """äº¤äº’å¼RAGæ¼”ç¤º"""
    
    print("=== AI Modular Blocks - ç®€å•RAGç³»ç»Ÿæ¼”ç¤º ===")
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    llm, vector_store = await setup_rag_system()
    if not llm or not vector_store:
        return
    
    # åŠ è½½æ–‡æ¡£
    await load_documents(vector_store)
    
    # é¢„è®¾é—®é¢˜æ¼”ç¤º
    demo_questions = [
        "Pythonæ˜¯ä»€ä¹ˆæ—¶å€™åˆ›å»ºçš„ï¼Ÿ",
        "å¦‚ä½•å®‰è£…PythonåŒ…ï¼Ÿ",
        "Pythonæ”¯æŒå“ªäº›ç¼–ç¨‹èŒƒå¼ï¼Ÿ",
        "Javaå’ŒC++çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ"  # è¿™ä¸ªé—®é¢˜æ–‡æ¡£ä¸­æ²¡æœ‰ç­”æ¡ˆ
    ]
    
    print("\nğŸ¯ å¼€å§‹æ¼”ç¤ºé¢„è®¾é—®é¢˜:")
    for question in demo_questions:
        await rag_query(question, llm, vector_store)
        print("-" * 50)
    
    print("\nâœ¨ RAGç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼")
    print("ğŸ’¡ è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†:")
    print("   1. å¦‚ä½•å°†æ–‡æ¡£å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“")
    print("   2. å¦‚ä½•æ ¹æ®é—®é¢˜æ£€ç´¢ç›¸å…³æ–‡æ¡£")
    print("   3. å¦‚ä½•å°†æ£€ç´¢ç»“æœä½œä¸ºä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”")


if __name__ == "__main__":
    asyncio.run(interactive_rag_demo())
